@article{10.1257/pandp.109.718,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2019},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {109},
  pages = {718--29},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.109.718},
  url = {http://www.aeaweb.org/articles?id=10.1257/pandp.109.718},
  urldate = {2019-09-21},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2021-02-06T16:57:48Z},
  file = {Full Text:/Users/larsvilhuber/Zotero/storage/587KRG5B/2019 - Report by the AEA Data Editor.pdf:application/pdf}
}

@article{American_Economic_Association2008-wayback,
  title = {Data Availability Policy},
  author = {{American Economic Association}},
  year = {2008},
  url = {https://web.archive.org/web/20180927113622/https://www.aeaweb.org/journals/policies/data-availability-policy},
  urldate = {2019-09-21},
  nonote = {(accessed: 2018-09-27 via Archive.org)},
  timestamp = {2019-12-22T21:40:29Z}
}

@article{ankel-peters2023,
  title = {Do Economists Replicate?},
  author = {{Ankel-Peters}, J{\"o}rg and Fiala, Nathan and Neubauer, Florian},
  year = {2023},
  month = aug,
  journal = {Journal of Economic Behavior \& Organization},
  volume = {212},
  pages = {219--232},
  issn = {0167-2681},
  doi = {10.1016/j.jebo.2023.05.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0167268123001555},
  urldate = {2023-10-09},
  abstract = {Reanalyses of empirical studies and replications in new contexts are important for scientific progress. Journals in economics increasingly require authors to provide data and code alongside published papers, but how much does the economics profession actually replicate? This paper summarizes existing replication definitions and reviews how much economists replicate other scholars' work. We argue that in order to counter incentive problems potentially leading to a replication crisis, replications in the spirit of Merton's `organized skepticism' are needed \textendash{} what we call `policing replications'. We review leading economics journals to show that policing replications are rare and conclude that more incentives to replicate are needed to reap the fruits of rising transparency standards.},
  keywords = {Generalizability,Meta-science,Replicability,Replication,Research transparency,Systematic review},
  timestamp = {2023-10-09T15:45:05Z},
  file = {ScienceDirect Snapshot:/Users/larsvilhuber/Zotero/storage/ITB4JPJZ/S0167268123001555.html:text/html}
}

@article{brodeur2023,
  title = {Replication Games: How to Make Reproducibility Research More Systematic},
  shorttitle = {Replication Games},
  author = {Brodeur, Abel and Dreber, Anna and {Hoces de la Guardia}, Fernando and Miguel, Edward},
  year = {2023},
  month = sep,
  journal = {Nature},
  volume = {621},
  number = {7980},
  pages = {684--686},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-023-02997-5},
  url = {https://www.nature.com/articles/d41586-023-02997-5},
  urldate = {2023-10-09},
  abstract = {In some areas of social science, around half of studies can't be replicated. A new test-fast, fail-fast initiative aims to show what research is hot \textemdash{} and what's not.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Economics,Research data,Research management,Scientific community,Sociology},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Research data, Research management, Scientific community, Economics, Sociology},
  timestamp = {2023-10-09T15:40:42Z},
  file = {Brodeur et al_2023_Replication games.pdf:/Users/larsvilhuber/Zotero/storage/EAXGH43Q/Brodeur et al_2023_Replication games.pdf:application/pdf}
}

@article{camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  url = {https://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2023-04-24},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Economics,Psychology},
  timestamp = {2023-04-24T00:53:58Z},
  file = {Camerer et al_2018_Evaluating the replicability of social science experiments in Nature and.pdf:/Users/larsvilhuber/Zotero/storage/CMGS3NRK/Camerer et al_2018_Evaluating the replicability of social science experiments in Nature and.pdf:application/pdf}
}

@article{cruwell2023,
  title = {What's in a {{Badge}}? {{A Computational Reproducibility Investigation}} of the {{Open Data Badge Policy}} in {{One Issue}} of {{Psychological Science}}},
  shorttitle = {What's in a {{Badge}}?},
  author = {Cr{\"u}well, Sophia and Apthorp, Deborah and Baker, Bradley J. and Colling, Lincoln and Elson, Malte and Geiger, Sandra J. and Lobentanzer, Sebastian and Mon{\'e}ger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas J. L.},
  year = {2023},
  month = apr,
  journal = {Psychological Science},
  volume = {34},
  number = {4},
  pages = {512--522},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/09567976221140828},
  url = {https://doi.org/10.1177/09567976221140828},
  urldate = {2023-10-19},
  abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
  langid = {english},
  timestamp = {2023-10-19T21:55:20Z},
  file = {Crüwell et al_2023_What’s in a Badge.pdf:/Users/larsvilhuber/Zotero/storage/ADQZW3A2/Crüwell et al_2023_What’s in a Badge.pdf:application/pdf}
}

@article{Glandon2011-ec,
  title = {Report on the {{American Economic Review Data Availability Compliance Project}}},
  author = {Glandon, Philip},
  year = {2011},
  journal = {Appendix to American Economic Review Editors Report},
  url = {https://web.archive.org/web/20130202231024/http://www.aeaweb.org/aer/2011_Data_Compliance_Report.pdf},
  timestamp = {2023-10-10T19:41:50Z},
  file = {Glandon_2011_Report on the American Economic Review Data Availability Compliance Project.pdf:/Users/larsvilhuber/Zotero/storage/SMDPNZ4H/Glandon_2011_Report on the American Economic Review Data Availability Compliance Project.pdf:application/pdf}
}

@article{HamermeshAm.Econ.Rev.2017,
  title = {Replication in {{Labor Economics}}: {{Evidence}} from {{Data}}, and {{What It Suggests}}},
  shorttitle = {Replication in {{Labor Economics}}},
  author = {Hamermesh, Daniel S.},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {37--40},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171121},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171121},
  urldate = {2018-05-22},
  abstract = {Examining the most heavily cited publications in labor economics from the early 1990s, I show that few of over 3,000 articles, citing them directly, replicates them. They are replicated more frequently using data from other time periods and economies, so that the validity of their central ideas has typically been verified. This pattern of scholarship suggests, beyond the currently required depositing of data and code upon publication, that there is little need for formal mechanisms for replication. The market for scholarship already produces replications of non-laboratory applied research.},
  langid = {english},
  timestamp = {2019-12-22T22:00:38Z},
  file = {Hamermesh_2017_Replication in Labor Economics.pdf:/Users/larsvilhuber/Zotero/storage/G35HY7W8/Hamermesh_2017_Replication in Labor Economics.pdf:application/pdf;Snapshot:/Users/larsvilhuber/Zotero/storage/RVD3DWI2/articles.html:text/html}
}

@article{huntington-klein2021,
  title = {The Influence of Hidden Researcher Decisions in Applied Microeconomics},
  author = {{Huntington-Klein}, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey R. and Burli, Pralhad and Chen, Naibin and Grieco, Paul and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin and Stopnitzky, Yaniv},
  year = {2021},
  journal = {Economic Inquiry},
  volume = {59},
  number = {3},
  pages = {944--960},
  issn = {1465-7295},
  doi = {10.1111/ecin.12992},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ecin.12992},
  urldate = {2021-09-27},
  abstract = {Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions. Two published causal empirical results are replicated by seven replicators each. We find large differences in data preparation and analysis decisions, many of which would not likely be reported in a publication. No two replicators reported the same sample size. Statistical significance varied across replications, and for one of the studies the effect's sign varied as well. The standard deviation of estimates across replications was 3\textendash 4 times the mean reported standard error.},
  langid = {english},
  keywords = {metascience,replication,research},
  timestamp = {2021-09-27T21:16:29Z},
  file = {Huntington-Klein et al_2021_The influence of hidden researcher decisions in applied microeconomics.pdf:/Users/larsvilhuber/Zotero/storage/3L262JHS/Huntington-Klein et al_2021_The influence of hidden researcher decisions in applied microeconomics.pdf:application/pdf;Snapshot:/Users/larsvilhuber/Zotero/storage/RP47YDY6/ecin.html:text/html}
}

@article{mas2019,
  title = {Report of the {{Editor}}: {{American Economic Journal}}: {{Applied Economics}}},
  shorttitle = {Report of the {{Editor}}},
  author = {Mas, Alex},
  year = {2019},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {109},
  pages = {639--645},
  issn = {2574-0768},
  doi = {10.1257/pandp.109.639},
  url = {https://www.aeaweb.org/articles?id=10.1257/pandp.109.639&ArticleSearch%5Bwithin%5D%5Barticletitle%5D=1&ArticleSearch%5Bwithin%5D%5Barticleabstract%5D=1&ArticleSearch%5Bwithin%5D%5Bauthorlast%5D=1&ArticleSearch%5Bq%5D=report&JelClass%5Bvalue%5D=0},
  urldate = {2023-10-08},
  langid = {english},
  timestamp = {2023-10-08T20:10:34Z},
  file = {Mas_2019_Report of the Editor.pdf:/Users/larsvilhuber/Zotero/storage/YL9ZC45C/Mas_2019_Report of the Editor.pdf:application/pdf}
}

@misc{menkveld2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Non-{{Standard Errors}}},
  author = {Menkveld, Albert J. and Dreber, Anna and Holzmeister, Felix and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Razen, Michael and Weitzel, Utz and Abad, David and Abudy, Menachem (Meni) and Adrian, Tobias and {Ait-Sahalia}, Yacine and Akmansoy, Olivier and Alcock, Jamie and Alexeev, Vitali and Aloosh, Arash and Amato, Livia and Amaya, Diego and Angel, James and Bach, Amadeus and Baidoo, Edwin and Bakalli, Gaetan and Barbon, Andrea and Bashchenko, Oksana and Bindra, Parampreet Christopher and Bjonnes, Geir Hoidal and Black, Jeff and Black, Bernard S. and Bohorquez, Santiago and Bondarenko, Oleg and Bos, Charles S. and {Bosch-Rosa}, Ciril and Bouri, Elie and Brownlees, Christian T. and Calamia, Anna and Cao, Viet Nga and {Capelle-Blancard}, Gunther and Capera, Laura and Caporin, Massimiliano and Carrion, Allen and Caskurlu, Tolga and Chakrabarty, Bidisha and Chernov, Mikhail and Cheung, William M. and Chincarini, Ludwig B. and Chordia, Tarun and Chow, Sheung Chi and Clapham, Benjamin and Colliard, Jean-Edouard and {Comerton-Forde}, Carole and Curran, Edward and Dao, Thong and Dare, Wale and Davies, Ryan J. and De Blasis, Riccardo and De Nard, Gianluca and Declerck, Fany and Deev, Oleg and Degryse, Hans and Deku, Solomon and Desagre, Christophe and {van Dijk}, Mathijs A. and Dim, Chukwuma and Dimpfl, Thomas and Dong, Yunjiang and Drummond, Philip and Dudda, Tom L. and Dumitrescu, Ariadna and Dyakov, Teodor and Dyhrberg, Anne Haubo and Dzieli{\'n}ski, Micha{\l} and Eksi, Asli and El Kalak, Izidin and {ter Ellen}, Saskia and Eugster, Nicolas and Evans, Martin D. D. and Farrell, Michael and {F{\'e}lez-Vi{\~n}as}, Ester and Ferrara, Gerardo and Ferrouhi, El Mehdi and Flori, Andrea and {Fluharty-Jaidee}, Jonathan and Foley, Sean and Fong, Kingsley Y. L. and Foucault, Thierry and Franus, Tatiana and Franzoni, Francesco A. and Frijns, Bart and Fr{\"o}mmel, Michael and Fu, Servanna and F{\"u}llbrunn, Sascha and Gan, Baoqing and Gehrig, Thomas and Gerritsen, Dirk and {Gil-Bazo}, Javier and Glosten, Lawrence R. and Gomez, Thomas and Gorbenko, Arseny and G{\"u}{\c c}bilmez, Ufuk and Grammig, Joachim and Gregoire, Vincent and Hagstr{\"o}mer, Bj{\"o}rn and Hambuckers, Julien and Hapnes, Erik and Harris, Jeffrey H. and Harris, Lawrence and Hartmann, Simon and Hasse, Jean-Baptiste and Hautsch, Nikolaus and He, Xuezhong and Heath, Davidson and Hediger, Simon and Hendershott, Terrence and Hibbert, Ann Marie and Hjalmarsson, Erik and Hoelscher, Seth A. and Hoffmann, Peter and Holden, Craig W. and Horenstein, Alex R. and Huang, Wenqian and Huang, Da and Hurlin, Christophe and Ivashchenko, Alexey and Iyer, Subramanian R. and Jahanshahloo, Hossein and Jalkh, Naji and Jones, Charles M. and Jurkatis, Simon and Jylha, Petri and Kaeck, Andreas and Kaiser, Gabriel and Karam, Arz{\'e} and Karmaziene, Egle and Kassner, Bernhard and Kaustia, Markku and Kazak, Ekaterina and Kearney, Fearghal and {van Kervel}, Vincent and Khan, Saad and Khomyn, Marta and Klein, Tony and Klein, Olga and Klos, Alexander and Koetter, Michael and Krahnen, Jan Pieter and Kolokolov, Aleksey and Korajczyk, Robert A. and Kozhan, Roman and Kwan, Amy and Lajaunie, Quentin and Lam, FY Eric and Lambert, Marie and Langlois, Hugues and Lausen, Jens and Lauter, Tobias and Leippold, Markus and Levin, Vladimir and Li, Yijie and Li, (Michael) Hui and Liew, Chee Yoong and Lindner, Thomas and Linton, Oliver B. and Liu, Jiacheng and Liu, Anqi and Llorente, Guillermo and Lof, Matthijs and Lohr, Ariel and Longstaff, Francis A. and {Lopez-Lira}, Alejandro and Mankad, Shawn and Mano, Nicola and Marchal, Alexis and Martineau, Charles and Mazzola, Francesco and Meloso, Debrah and Mihet, Roxana and Mohan, Vijay and Moinas, Sophie and Moore, David and Mu, Liangyi and Muravyev, Dmitriy and Murphy, Dermot and Neszveda, Gabor and Neumeier, Christian and Nielsson, Ulf and Nimalendran, Mahendrarajah and Nolte, Sven and Norden, Lars L. and O'Neill, Peter and Obaid, Khaled and {\O}degaard, Bernt Arne and {\"O}stberg, Per and Painter, Marcus and Palan, Stefan and Palit, Imon and Park, Andreas and Pascual, Roberto and Pasquariello, Paolo and Pastor, Lubos and Patel, Vinay and Patton, Andrew J. and Pearson, Neil D. and Pelizzon, Loriana and Pelster, Matthias and P{\'e}rignon, Christophe and Pfiffer, Cameron and Philip, Richard and Pl{\'i}hal, Tom{\'a}{\v s} and Prakash, Puneet and Press, Oliver-Alexander and Prodromou, Tina and Putni{\c n}{\v s}, T{\=a}lis J. and Raizada, Gaurav and Rakowski, David A. and Ranaldo, Angelo and Regis, Luca and Reitz, Stefan and Renault, Thomas and Renjie, Rex Wang and Ren{\`o}, Roberto and Riddiough, Steven and Rinne, Kalle and Rintam{\"a}ki, Paul and Riordan, Ryan and Rittmannsberger, Thomas and {Rodr{\'i}guez-Longarela}, I{\~n}aki and R{\"o}sch, Dominik and Rognone, Lavinia and Roseman, Brian and Rosu, Ioanid and Roy, Saurabh and Rudolf, Nicolas and Rush, Stephen and Rzayev, Khaladdin and Rze{\'z}nik, Aleksandra and Sanford, Anthony and Sankaran, Harikumar and Sarkar, Asani and Sarno, Lucio and Scaillet, O. and Scharnowski, Stefan and {Schenk-Hopp{\'e}}, Klaus Reiner and Schertler, Andrea and Schneider, Michael and Schroeder, Florian and Schuerhoff, Norman and Schuster, Philipp and Schwarz, Marco A. and Seasholes, Mark S. and Seeger, Norman and Shachar, Or and Shkilko, Andriy and Shui, Jessica and Sikic, Mario and Simion, Giorgia and Smales, Lee A. and S{\"o}derlind, Paul and Sojli, Elvira and Sokolov, Konstantin and Spokeviciute, Laima and Stefanova, Denitsa and Subrahmanyam, Marti G. and Neus{\"u}ss, Sebastian and Szaszi, Barnabas and Talavera, Oleksandr and Tang, Yuehua and Taylor, Nicholas and Tham, Wing Wah and Theissen, Erik and Thimme, Julian and Tonks, Ian and Tran, Hai and Trapin, Luca and Trolle, Anders B. and Valente, Giorgio and Van Ness, Robert A. and Vasquez, Aurelio and Verousis, Thanos and Verwijmeren, Patrick and Vilhelmsson, Anders and Vilkov, Grigory and Vladimirov, Vladimir and Vogel, Sebastian and Voigt, Stefan and Wagner, Wolf and Walther, Thomas and Weiss, Patrick and {van der Wel}, Michel and Werner, Ingrid M. and Westerholm, P. Joakim and Westheide, Christian and Wipplinger, Evert and Wolf, Michael and Wolff, Christian C. P. and Wolk, Leonard and Wong, Wing-Keung and Wrampelmeyer, Jan and Xia, Shuo and Xiu, Dacheng and Xu, Ke and Xu, Caihong and Yadav, Pradeep K. and Yag{\"u}e, Jos{\'e} and Yan, Cheng and Yang, Antti and Yoo, Woongsun and Yu, Wenjia and Yu, Shihao and Yueshen, Bart Zhou and Yuferova, Darya and Zamojski, Marcin and Zareei, Abalfazl and Zeisberger, Stefan and Zhang, S. Sarah and Zhang, Xiaoyu and Zhong, Zhuo and Zhou, Z. Ivy and Zhou, Chen and Zhu, Xingyu Sonya and Zoican, Marius and Zwinkels, Remco C. J. and Chen, Jian and Duevski, Teodor and Gao, Ge and Gemayel, Roland and Gilder, Dudley and Kuhle, Paul and Pagnotta, Emiliano and Pelli, Michele and S{\"o}nksen, Jantje and Zhang, Lu and Ilczuk, Konrad and Bogoev, Dimitar and Qian, Ya and Wika, Hans C. and Yu, Yihe and Zhao, Lu and Mi, Michael and Bao, Li and Vaduva, Andreea and Prokopczuk, Marcel and Avetikian, Alejandro and Wu, Zhen-Xing},
  year = {2023},
  month = may,
  number = {3961574},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.3961574},
  url = {https://papers.ssrn.com/abstract=3961574},
  urldate = {2023-10-09},
  abstract = {In statistics, samples are drawn from a population in a data-generating process (DGP). Standard errors measure the uncertainty in estimates of population parameters. In science, evidence is generated to test hypotheses in an evidence-generating process (EGP). We claim that EGP variation across researchers adds uncertainty: Non-standard errors (NSEs). We study NSEs by letting 164 teams test the same hypotheses on the same data. NSEs turn out to be sizable, but smaller for better reproducible or higher rated research. Adding peer-review stages reduces NSEs. We further find that this type of uncertainty is underestimated by participants.},
  langid = {english},
  keywords = {liquidity,multi-analyst approach,non-standard errors},
  timestamp = {2023-10-09T03:12:18Z},
  file = {Menkveld et al_2023_Non-Standard Errors.pdf:/Users/larsvilhuber/Zotero/storage/A24XSCZQ/Menkveld et al_2023_Non-Standard Errors.pdf:application/pdf}
}

@misc{nagel2018,
  title = {Code-Sharing Policy: Update},
  shorttitle = {Code-Sharing Policy},
  author = {Nagel, Stefan},
  year = {2018},
  month = mar,
  journal = {Journal of Finance},
  url = {https://voices.uchicago.edu/jfeditor/2018/03/06/code-sharing-policy-update/},
  urldate = {2023-10-09},
  abstract = {In September 2016, the JF adopted a code-sharing policy. The first accepted papers with replication code are now online (scroll down and click on supporting information): Reproducibility of researc\ldots},
  langid = {american},
  timestamp = {2023-10-09T15:09:09Z},
  file = {Snapshot:/Users/larsvilhuber/Zotero/storage/2S8WBMLK/code-sharing-policy-update.html:text/html}
}

@book{nasem2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2019},
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/25303},
  url = {https://doi.org/10.17226/25303},
  urldate = {2019-09-21},
  isbn = {978-0-309-48616-3},
  timestamp = {2023-10-08T16:25:08Z}
}

@article{obels2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Dani{\"e}l and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {229--237},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  url = {https://doi.org/10.1177/2515245920918872},
  urldate = {2023-10-19},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english},
  timestamp = {2023-10-19T21:55:07Z},
  file = {Obels et al_2020_Analysis of Open Data and Computational Reproducibility in Registered Reports.pdf:/Users/larsvilhuber/Zotero/storage/QNTN87C8/Obels et al_2020_Analysis of Open Data and Computational Reproducibility in Registered Reports.pdf:application/pdf}
}

@misc{openalex2022,
  title = {{{OpenAlex}}: {{A}} Fully-Open Index of Scholarly Works, Authors, Venues, Institutions, and Concepts},
  shorttitle = {{{OpenAlex}}},
  author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01833},
  eprint = {2205.01833},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.01833},
  url = {http://arxiv.org/abs/2205.01833},
  urldate = {2023-10-12},
  abstract = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Digital Libraries},
  timestamp = {2023-10-12T14:02:20Z},
  file = {Priem et al_2022_OpenAlex.pdf:/Users/larsvilhuber/Zotero/storage/8C3GIA7Y/Priem et al_2022_OpenAlex.pdf:application/pdf;arXiv.org Snapshot:/Users/larsvilhuber/Zotero/storage/8WXGRYL5/2205.html:text/html}
}

@article{opensciencecollaboration2015b,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {https://www.science.org/doi/10.1126/science.aac4716},
  urldate = {2023-04-24},
  abstract = {Empirically analyzing empirical evidence                            One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts               et al.               describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.                                         Science               , this issue               10.1126/science.aac4716                        ,              A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.           ,                             INTRODUCTION               Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.                                         RATIONALE               There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.                                         RESULTS                                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and                 P                 values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (                 M                 r                 = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (                 M                 r                 = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (                 P                 {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.                                                        CONCLUSION                                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original                 P                 value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.                              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.                                                   Original study effect size versus replication effect size (correlation coefficients).                   Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.                                                                         ,              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  timestamp = {2023-04-24T01:02:27Z},
  file = {Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf:/Users/larsvilhuber/Zotero/storage/M9QIGGBL/Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf:application/pdf}
}

@misc{ourresearch2023,
  title = {{{OpenAlex}}},
  author = {{OurResearch}},
  year = {2023},
  url = {https://openalex.org/about},
  urldate = {2023-10-12},
  timestamp = {2023-10-12T14:01:59Z}
}

@misc{perignon2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Computational {{Reproducibility}} in {{Finance}}: {{Evidence}} from 1,000 {{Tests}}},
  shorttitle = {Computational {{Reproducibility}} in {{Finance}}},
  author = {P{\'e}rignon, Christophe and Akmansoy, Olivier and Hurlin, Christophe and Dreber, Anna and Holzmeister, Felix and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Menkveld, Albert J. and Razen, Michael and Weitzel, Utz},
  year = {2023},
  month = jul,
  number = {4064172},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.4064172},
  url = {https://papers.ssrn.com/abstract=4064172},
  urldate = {2023-09-27},
  abstract = {We analyze the computational reproducibility of more than 1,000 empirical answers to six research questions in finance provided by 168 international research teams. Running the original researchers' code on the same raw data regenerates exactly the same results only 52\% of the time. Reproducibility is higher for researchers with better coding skills and for those exerting more effort. It is lower for more technical research questions, more complex code, and for results lying in the tails of the results distribution. Neither researcher seniority, nor peer-review ratings appear to be related to the level of reproducibility. Moreover, researchers exhibit strong overconfidence when assessing the reproducibility of their own research. We provide guidelines for finance researchers and discuss several implementable reproducibility policies for academic journals.},
  langid = {english},
  keywords = {computational reproducibility,credibility of research,data-availability policy,multi-analyst study,open science,scientific publishing},
  timestamp = {2023-09-27T20:06:58Z},
  file = {Pérignon et al_2023_Computational Reproducibility in Finance.pdf:/Users/larsvilhuber/Zotero/storage/Z69Q6FKW/Pérignon et al_2023_Computational Reproducibility in Finance.pdf:application/pdf}
}

@article{serra-garcia2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  journal = {Science Advances},
  volume = {7},
  number = {21},
  pages = {eabd1705},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  url = {https://advances.sciencemag.org/content/7/21/eabd1705},
  urldate = {2021-05-22},
  abstract = {We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility. Published papers that fail to replicate are cited more than those that replicate, even after the failure is published. Published papers that fail to replicate are cited more than those that replicate, even after the failure is published.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. https://creativecommons.org/licenses/by-nc/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  timestamp = {2021-05-22T01:50:48Z},
  file = {Serra-Garcia_Gneezy_2021_Nonreplicable publications are cited more than replicable ones.pdf:/Users/larsvilhuber/Zotero/storage/AZCIS2ER/Serra-Garcia_Gneezy_2021_Nonreplicable publications are cited more than replicable ones.pdf:application/pdf;Snapshot:/Users/larsvilhuber/Zotero/storage/ZTM9ZAYD/eabd1705.html:text/html}
}

@article{templateREADMEv1.0,
  title = {A Template {{README}} for Social Science Replication Packages},
  author = {Vilhuber, Lars and Connolly, Marie and Koren, Mikl{\'o}s and Llull, Joan and Morrow, Peter},
  year = {2020},
  month = dec,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.4319999},
  url = {https://zenodo.org/record/4319999},
  urldate = {2021-04-01},
  abstract = {The typical README in social science journals serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper, including the description of the origins of data and/or description of programs. As such, a good README file should first provide a brief overview of the available material and a brief guide as to how to proceed from beginning to end, before then diving into the specifics. These template files structure such a README in a way that is compliant with the typical data and code workflow in the social sciences.},
  copyright = {Creative Commons Attribution Non Commercial 4.0 International, Open Access},
  langid = {english},
  keywords = {economics,reproducibility,social sciences},
  timestamp = {2023-10-10T21:34:59Z}
}

@techreport{templateREADMEv1.1,
  title = {A Template {{README}} for Social Science Replication Packages},
  author = {Vilhuber, Lars and Connolly, Marie and Koren, Mikl{\'o}s and Llull, Joan and Morrow, Peter},
  year = {2022},
  month = nov,
  number = {v1.1.0},
  institution = {{Zenodo}},
  url = {https://doi.org/10.5281/zenodo.7293838},
  urldate = {2023-05-17},
  abstract = {The typical README in social science journals serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper, including the description of the origins of data and/or description of programs. As such, a good README file should first provide a brief overview of the available material and a brief guide as to how to proceed from beginning to end, before then diving into the specifics. These template files structure such a README in a way that is compliant with the typical data and code workflow in the social sciences.},
  keywords = {economics,reproducibility,social sciences},
  timestamp = {2023-10-10T21:34:20Z},
  file = {Zenodo Snapshot:/Users/larsvilhuber/Zotero/storage/RX257YKW/7293838.html:text/html}
}

@article{vilhuber2020a,
  title = {Reproducibility and {{Replicability}} in {{Economics}}},
  author = {Vilhuber, Lars},
  year = {2020},
  month = dec,
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {4},
  doi = {10.1162/99608f92.4f6b9e67},
  url = {https://hdsr.mitpress.mit.edu/pub/fgpmpj1l},
  copyright = {All rights reserved},
  timestamp = {2023-10-10T21:35:41Z}
}

@article{vilhuber2021,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2021},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {111},
  pages = {808--817},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.111.808},
  url = {https://pubs.aeaweb.org/doi/10.1257/pandp.111.808},
  urldate = {2021-05-20},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2021-05-20T14:42:52Z},
  file = {2021_Report by the AEA Data Editor.pdf:/Users/larsvilhuber/Zotero/storage/TL7UKNWC/2021_Report by the AEA Data Editor.pdf:application/pdf}
}

@article{vilhuber2022a,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2022},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {112},
  pages = {813--23},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.112.813},
  langid = {english},
  timestamp = {2022-06-01T15:52:55Z}
}

@article{vilhuber2022b,
  ids = {vilhuber2022f},
  title = {Teaching for Large-Scale {{Reproducibility Verification}}},
  author = {Vilhuber, Lars and Son, Hyuk Harry and Welch, Meredith and Wasser, David N. and Darisse, Michael},
  year = {2022},
  month = sep,
  journal = {Journal of Statistics and Data Science Education},
  volume = {30},
  number = {3},
  pages = {274--281},
  doi = {10.1080/26939169.2022.2074582},
  url = {https://arxiv.org/abs/2204.01540v1},
  abstract = {We describe a unique environment in which undergraduate students from various STEM and social science disciplines are trained in data provenance and reproducible methods, and then apply that knowledge to real, conditionally accepted manuscripts and associated replication packages. We describe in detail the recruitment, training, and regular activities. While the activity is not part of a regular curriculum, the skills and knowledge taught through explicit training of reproducible methods and principles, and reinforced through repeated application in a real-life workflow, contribute to the education of these undergraduate students, and prepare them for post-graduation jobs and further studies.},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2022-12-07T02:46:40Z},
  file = {Vilhuber et al_2022_Teaching for Large-Scale Reproducibility Verification.pdf:/Users/larsvilhuber/Zotero/storage/IKKSYQFN/Vilhuber et al_2022_Teaching for Large-Scale Reproducibility Verification.pdf:application/pdf}
}

@article{VilhuberAEAPap.Proc.2020,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars and Turitto, James and Welch, Keesler},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {764--75},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.110.764},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2020-11-21T21:59:57Z}
}

@article{vlaeminck2021,
  title = {Dawning of a New Age? {{Economics}} Journals' Data Policies on the Test Bench},
  shorttitle = {Dawning of a New Age?},
  author = {Vlaeminck, Sven},
  year = {2021},
  month = aug,
  journal = {LIBER Quarterly: The Journal of the Association of European Research Libraries},
  volume = {31},
  number = {1},
  pages = {1--29},
  issn = {2213-056X},
  doi = {10.53377/lq.10940},
  url = {https://liberquarterly.eu/article/view/10940},
  urldate = {2021-08-31},
  abstract = {In the field of social sciences and particularly in economics, studies have frequently reported a lack of reproducibility of published research. Most often, this is due to the unavailability of data reproducing the findings of a study. However, over the past years, debates on open science practices and reproducible research have become stronger and louder among research funders, learned societies, and research organisations. Many of these have started to implement data policies to overcome these shortcomings. Against this background, the article asks if there have been changes in the way economics journals handle data and other materials that are crucial to reproduce the findings of empirical articles. For this purpose, all journals listed in the Clarivate Analytics Journal Citation Reports edition for economics have been evaluated for policies on the disclosure of research data. The article describes the characteristics of these data policies and explicates their requirements. Moreover, it compares the current findings with the situation some years ago. The results show significant changes in the way journals handle data in the publication process. Research libraries can use the findings of this study for their advisory activities to best support researchers in submitting and providing data as required by journals.},
  copyright = {Copyright (c) 2021 Sven Vlaeminck},
  langid = {english},
  keywords = {data policies,economics,journals,open science,reproducibility},
  timestamp = {2021-08-31T17:27:40Z}
}

@inproceedings{wang2020,
  title = {Assessing and Restoring Reproducibility of {{Jupyter}} Notebooks},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller, Andreas},
  year = {2020},
  month = dec,
  pages = {138--149},
  publisher = {{ACM}},
  address = {{Virtual Event Australia}},
  doi = {10.1145/3324884.3416585},
  url = {https://dl.acm.org/doi/10.1145/3324884.3416585},
  urldate = {2023-03-22},
  isbn = {978-1-4503-6768-4},
  langid = {english},
  timestamp = {2023-03-22T18:54:05Z},
  file = {Wang et al_2020_Assessing and restoring reproducibility of Jupyter notebooks.pdf:/Users/larsvilhuber/Zotero/storage/MGUTI7A5/Wang et al_2020_Assessing and restoring reproducibility of Jupyter notebooks.pdf:application/pdf}
}

@article{welch2019c,
  title = {Reproducing, {{Extending}}, {{Updating}},{{Replicating}}, {{Reexamining}}, and {{Reconciling}}},
  author = {Welch, Ivo},
  year = {2019},
  month = dec,
  journal = {Critical Finance Review},
  volume = {8},
  number = {1-2},
  pages = {301--304},
  issn = {21645744, 21645760},
  doi = {10.1561/104.00000082},
  url = {http://www.nowpublishers.com/article/Details/CFR-0082},
  urldate = {2023-10-09},
  annotation = {https://cfr.pub/published/papers/welch2020reproducing.pdf},
  timestamp = {2023-10-09T15:05:34Z}
}
