f% Encoding: UTF-8


@Misc{DOI2012,
  Title                    = {The {D}igital {O}bject {I}dentifier System Home Page},
  Author                   = {{International DOI Foundation (IDF)}},
  URL                      = {http://www.doi.org},
  urldate                     = {2014-10-08},
  year                    = 2012,
  Keywords = {InText},
}

@TechReport{AndersonEtAl2005,
  Title                    = {The Role of Data and Program Code Archives in the Future of Economic Research},
  Author                   = {Richard G. Anderson and William H. Greene and Bruce D. McCullough and H.D. Vinod},
  Year                     = {2005},
  Number                   = {2005-014C},
  Type                     = {Working Paper},
  Organization             = {Federal Reserve Bank of St. Louis},
  Keywords = {InText},
}

@Article{Baker_2015,
  author    = {Monya Baker},
  title     = {Over half of psychology studies fail reproducibility test},
  journal   = {Nature},
  year      = {2015},
  month     = {aug},
  doi       = {10.1038/nature.2015.18248},
  keywords  = {InText},
  owner     = {vilhuber},
  publisher = {Nature Publishing Group},
  timestamp = {2015.11.05},
  url       = {http://dx.doi.org/10.1038/nature.2015.18248},
}

@Misc{BellMiller2013b,
  Title                    = {How to Persuade Journals to Accept Your Replication Paper},
  Author                   = {Mark Bell and Nicholas Miller},
  URL                      = {https://politicalsciencereplication.wordpress.com/2013/09/11/guest-blog-how-to-persuade-journals-to-accept-your-replication-paper/},
  Urldate                  = {2014-10-08},
  Year                     = {2013},
  Keywords = {InText},
}

@Article{BellMiller2013,
  Title                    = {Questioning the Effect of Nuclear Weapons on Conflict},
  Author                   = {Bell, Mark S. and Miller, Nicholas L.},
  Journal                  = {Journal of Conflict Resolution},
  Year                     = {2013},
  Abstract                 = {We examine the effect of nuclear weapons on interstate conflict. Using more appropriate methodologies than have previously been used, we find that dyads in which both states possess nuclear weapons are not significantly less likely to fight wars, nor are they significantly more or less belligerent at low levels of conflict. This stands in contrast to previous work, which suggests nuclear dyads are some 2.7 million times less likely to fight wars. We additionally find that dyads in which one state possesses nuclear weapons are more prone to low-level conflict (but not more prone to war). This appears to be because nuclear-armed states expand their interests after nuclear acquisition rather than because nuclear weapons provide a shield behind which states can aggress against more powerful conventional-armed states. This calls into question conventional wisdom on the impact of nuclear weapons and has policy implications for the impact of nuclear proliferation.},
  DOI                      = {10.1177/0022002713499718},
  Eprint                   = {http://jcr.sagepub.com/content/early/2013/08/19/0022002713499718.full.pdf+html},
  URL                      = {http://jcr.sagepub.com/content/early/2013/08/19/0022002713499718.abstract},
  Keywords = {InText},
}

@Article{BurmanEtAl2010,
  Title                    = {A Call for Replication Studies},
  Author                   = {Burman, Leonard E. and Reed, W. Robert and Alm, James},
  Journal                  = {Public Finance Review},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {787-793},
  Volume                   = {38},
  DOI                      = {10.1177/1091142110385210},
  Eprint                   = {http://pfr.sagepub.com/cgi/reprint/38/6/787},
  URL                      = {http://pfr.sagepub.com/cgi/content/short/38/6/787},
  Keywords = {InText},
}

@TECHREPORT{ChangLi2015,
title = {Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say "Usually Not"},
author = {Chang, Andrew C. and Li, Phillip},
year = {2015},
institution = {Board of Governors of the Federal Reserve System (U.S.)},
type = {Finance and Economics Discussion Series},
url = {https://www.federalreserve.gov/econresdata/feds/2015/files/2015083pap.pdf},
number = {2015-83},
  Keywords = {InText},
}

@Article{PsyRep2015,
  Title                    = {Estimating the reproducibility of psychological science},
  Author                   = {Open Science Collaboration},
  Journal                  = {Science},
  Year                     = {2015},
  Month                    = {aug},
  Number                   = {6251},
  Pages                    = {aac4716--aac4716},
  Volume                   = {349},
  DOI                      = {10.1126/science.aac4716},
  Owner                    = {vilhuber},
  Publisher                = {American Association for the Advancement of Science ({AAAS})},
  Timestamp                = {2015.11.05},
  URL                      = {http://dx.doi.org/10.1126/science.aac4716},
  Keywords = {InText},
}

@Article{Dewald1986,
  author={Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  title={{Replication in Empirical Economics: The Journal of Money, Credit and Banking Project}},
  journal={American Economic Review},
  year=1986,
  volume={76},
  number={4},
  pages={587-603},
  Keywords = {InText},
}

@Article{Duvendack2015,
title = {Replications in Economics: A Progress Report},
author = {Duvendack, Maren and Palmer-Jones, Richard W. and Reed, W.},
year = {2015},
volume = 12,
number = 2,
pages = {164-191},
journal={ECON Journal Watch},
keywords = {InText},
}

@article{Frisch1933,
 author = {Ragnar Frisch},
 journal = {Econometrica},
 number = {1},
 pages = {1-4},
 publisher = {[Wiley, Econometric Society]},
 title = {Editor's Note},
 volume = {1},
 year = {1933},
  Keywords = {InText},
}

@Article{Glandon2010,
  author = {Glandon, Philip},
  title = {Report on the American Economic Review Data Availability
Compliance Project},
Journal = {Appendix to American Economic Review Editors Report},
  url = {http://www.aeaweb.org/aer/2011_Data_Compliance_Report.pdf},
  year = 2011,
  Keywords = {InText},
}

@TechReport{NBERw21754,
  author =      {Daniel S. Hamermesh},
  title =       {Citations in Economics: Measurement, Uses and Impacts},
  institution = {National Bureau of Economic Research},
  year =        {2015},
  type =        {Working Paper},
  number =      {21754},
  month =       {November},
  abstract =    {I describe and compare sources of data on citations in economics and the statistics that can be constructed from them. Constructing data sets of the post-publication citation histories of articles published in the “Top 5” journals in the 1970s and the 2000s, I examine distributions and life cycles of citations, compare citation histories of articles in different sub-specialties in economics and present evidence on the history and heterogeneity of those journals’ impacts and the marginal citation productivity of additional coauthors. I use a new data set of the lifetime citation histories of over 1000 economists from 30 universities to rank economics departments by various measures and to demonstrate the importance of intra- and inter-departmental heterogeneity in productivity. Throughout, the discussion summarizes earlier work. I survey research on the impacts of citations on salaries and non-monetary rewards and discuss how citations reflect judgments about research quality in economics.},
  doi =         {10.3386/w21754},
  owner =       {vilhuber},
  series =      {Working Paper Series},
  timestamp =   {2016.01.17},
  url =         {http://www.nber.org/papers/w21754},
  Keywords = {InText},
}

@Article{King95,
  Title                    = {Replication, Replication},
  Author                   = {Gary King},
  Journal                  = {PS: Political Science and Politics},
  Year                     = {1995},
  Month                    = sep,
  Number                   = {3},
  Pages                    = {443--499},
  Volume                   = {28},
  Abstract                 = {Political science is a community enterprise and the community of empirical political scientists need access to the body of data necessary to replicate existing studies to understand, evaluate, and especially build on this work. Unfortunately, the norms we have in place now do not encourage, or in some cases even permit, this aim. Following are suggestions that would facilitate replication and are easy to implement  by teachers, students, dissertation writers, graduate programs, authors, reviewers, funding agencies, and journal and book editors.},
  Keywords = {InText},
}

@Article{McCullough2007,
  Title                    = {Got Replicability? The Journal of Money, Credit and Banking Archive},
  Author                   = {B.D. {McCullough}},
  Year                     = {2007},
  Month                    = sep,
  Number                   = {3},
  Pages                    = {326-337},
  Volume                   = {4},
  Journaltitle             = {Econ Journal Watch},
  URL                      = {http://www.econjournalwatch.org/pdf/McCulloughEconomicsInPracticeSeptember2007.pdf},
  Urldate                  = {2014-10-08},
  Keywords = {InText},
}

@Article{McCullough2006,
  Title                    = {{Lessons from the JMCB Archive}},
  Author                   = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
  Journal                  = {Journal of Money, Credit and Banking},
  Year                     = {2006},
  Month                    = jun,
  Number                   = {4},
  Pages                    = {1093-1107},
  Volume                   = {38},
  Abstract                 = {We examine the online archive of the Journal of Money, Credit, and Banking, in which an author is required to deposit the data and code that replicate the results of his paper. We find that most authors do not fulfill this requirement. Of more than 150 empirical articles, fewer than 15 could be replicated. Despite all this, there is no doubt that a data/code archive is more conducive to replicable research than the alternatives. We make recommendations to improve the functioning of the archive.},
  Keywords                 = {InText},
  URL                      = {http://ideas.repec.org/a/mcb/jmoncb/v38y2006i4p1093-1107.html}
}

@Article{McCullough03,
  Title                    = {Econometrics and Software: Comments},
  Author                   = {McCullough, B D and Vinod, Hrishikesh D.},
  Journal                  = {Journal of Economic Perspectives},
  Year                     = {2003},
  Number                   = {1},
  Pages                    = {223-224},
  Volume                   = {17},
  URL                      = {http://EconPapers.repec.org/RePEc:aea:jecper:v:17:y:2003:i:1:p:223-224},
  Keywords = {InText},
}

@Misc{ReplicationWiki,
  author =    {Multiple},
  title =     {ReplicationWiki for AEJ:AE},
  year =      {2016},
  owner =     {vilhuber},
  timestamp = {2016.01.24},
  url =       {http://replication.uni-goettingen.de/wiki/index.php/Category:AEJ:AE},
  urldate =   {2016-01-24},
  Keywords = {InText},
}

@Article{NEB2014,
  author    = {{Nature Editorial Board}},
  title     = {Journals unite for reproducibility},
  journal   = {Nature},
  year      = {2014},
  volume    = {515},
  number    = {7525},
  pages     = {7--7},
  month     = {nov},
  doi       = {10.1038/515007a},
  keywords  = {InText},
  owner     = {vilhuber},
  publisher = {Nature Publishing Group},
  timestamp = {2015.11.05},
  url       = {http://dx.doi.org/10.1038/515007a},
}

@Article{PollardWilkinson2010,
  Title                    = {Making Datasets Visible and Accessible: DataCite's First Summer Meeting},
  Author                   = {Tom J. Pollard and {J. Max} Wilkinson},
  Journal                  = {Ariadne},
  Year                     = {2010},
  Volume                   = {64},
  URL                      = {http://www.ariadne.ac.uk/issue64/datacite-2010-rpt},
  Urldate                  = {2014-10-08},
  Keywords = {InText},
}

@Article{Schooler_2014,
  author    = {Jonathan W. Schooler},
  title     = {Metascience could rescue the `replication crisis'},
  journal   = {Nature},
  year      = {2014},
  volume    = {515},
  number    = {7525},
  pages     = {9--9},
  month     = {nov},
  doi       = {10.1038/515009a},
  keywords  = {InText},
  owner     = {vilhuber},
  publisher = {Nature Publishing Group},
  timestamp = {2015.11.05},
  url       = {http://dx.doi.org/10.1038/515009a},
}

@Misc{Handle,
  Title                    = {Handle System Overview},
  Author                   = {Sun, S. X. and Lannom, L. and Boesch, B.},
  Organization             = {Corporation for National Research Initiaties},
  URL                      = {http://www.handle.net/factsheet.html},
  Year                     = {2010},
  Keywords = {InText},
}

@Misc{WebofScience,
  author =    {Thomson-Reuters},
  title =     {Web of Science},
  year =      {2016},
  owner =     {vilhuber},
  timestamp = {2016.01.24},
  url =       {http://apps.webofknowledge.com},
  urldate =   {2016-01-24},
  Keywords = {InText},
}

@InProceedings{Vinod2005,
  Title                    = {Evaluation of Archived Code with Perturbation Checks and Alternatives},
  Author                   = {Hrishikesh D. Vinod},
  Booktitle                = {Meetings of the American Economic Association},
  Year                     = {2005},
  URL                      = {https://www.aeaweb.org/annual_mtg_papers/2005/0109_1300_0304.pdf},
  Urldate                  = {2014-10-08},
  Keywords = {InText},
}



@TechReport{RePEc:fip:fedlwp:2015-016,
  author={Zimmermann, Christian},
  title={{On the Need for a Replication Journal}},
  year=2015,
  month=Aug,
  institution={Federal Reserve Bank of St. Louis},
  type={Working Papers},
  url={https://ideas.repec.org/p/fip/fedlwp/2015-016.html},
  number={2015-16},
  abstract={There is very little replication of research in economics, particularly compared with other sciences. This paper argues that there is a dire need for studies that replicate research, that their scarcity is due to poor or negative rewards for replicators, and that this could be improved with a journal that exclusively publishes replication studies. I then discuss how such a journal could be organized, in particular in the face of some negative rewards some replication studies may elicit.},
  Keywords = {InText},
  doi={},
}

@article{Hirsch2005,
author = {Hirsch, J. E.},
title = {An index to quantify an individual's scientific research output},
volume = {102},
number = {46},
pages = {16569-16572},
year = {2005},
doi = {10.1073/pnas.0507655102},
abstract ={I propose the index h, defined as the number of papers with citation number larger than h, as a useful index to characterize the scientific output of a researcher.},
URL = {http://www.pnas.org/content/102/46/16569.abstract},
eprint = {http://www.pnas.org/content/102/46/16569.full.pdf},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
  Keywords = {InText},
}

### DEFINITION OF REPRODUCIBILITY / REPLICABILITY

@TECHREPORT{Bollen2015-vb,
  title       = "Social, Behavioral, and Economic Sciences Perspectives on
                 Robust and Reliable Science",
  author      = "Bollen, Kenneth and Cacioppo, John T and Kaplan, Robert M and
                 Korsnick, Jon A and Olds, James L",
  institution = "Subcommittee on Replicability in Science, National Science
                 Foundation Directorate for Social, Behavioral, and Economic
                 Sciences",
  month       =  may,
  year        =  2015
}

@article{10.1257/pandp.108.745,
	Author = {Duflo, Esther and Hoynes, Hilary},
	Title = {Report of the Search Committee to Appoint a Data Editor for the AEA},
	Journal = {AEA Papers and Proceedings},
	Volume = {108},
	Year = {2018},
	Pages = {745},
	DOI = {10.1257/pandp.108.745},
	URL = {http://www.aeaweb.org/articles?id=10.1257/pandp.108.745}}



@MISC{Fuentes2016-wz,
	title        = "Reproducible Research in {JASA}",
	author       = "Fuentes, Montse",
	abstract     = "JASA is leading the effort to establish publication standards
	that improve research quality and reproducibility.",
	month        =  jul,
	year         =  2016,
	howpublished = "\url{http://magazine.amstat.org/blog/2016/07/01/jasa-reproducible16/}",
	note         = "Accessed: 2017-4-4"
}

@Article{Stodden2018,
  author    = {Victoria Stodden and Jennifer Seiler and Zhaokun Ma},
  title     = {An empirical analysis of journal policy effectiveness for computational reproducibility},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2018},
  volume    = {115},
  number    = {11},
  pages     = {2584--2589},
  month     = mar,
  doi       = {10.1073/pnas.1708290115},
  owner     = {vilhuber},
  publisher = {Proceedings of the National Academy of Sciences},
  timestamp = {2018.11.26},
}

@Article{Stark2018,
  author    = {Philip B. Stark},
  title     = {Before reproducibility must come preproducibility},
  journal   = {Nature},
  year      = {2018},
  volume    = {557},
  number    = {7707},
  pages     = {613--613},
  month     = may,
  doi       = {10.1038/d41586-018-05256-0},
  owner     = {vilhuber},
  publisher = {Springer Nature},
  timestamp = {2018.11.26},
}

@Book{NAP25116,
  title         = {Open Science by Design: Realizing a Vision for 21st Century Research},
  publisher     = {The National Academies Press},
  year          = {2018},
  author        = {{National Academies of Sciences, Engineering, and Medicine}},
  address       = {Washington, DC},
  isbn          = {978-0-309-47624-9},
  __markedentry = {[vilhuber:]},
  abstract      = {Openness and sharing of information are fundamental to the progress of science and to the effective functioning of the research enterprise. The advent of scientific journals in the 17th century helped power the Scientific Revolution by allowing researchers to communicate across time and space, using the technologies of that era to generate reliable knowledge more quickly and efficiently. Harnessing today\u2019s stunning, ongoing advances in information technologies, the global research enterprise and its stakeholders are moving toward a new open science ecosystem. Open science aims to ensure the free availability and usability of scholarly publications, the data that result from scholarly research, and the methodologies, including code or algorithms, that were used to generate those data.\n\nOpen Science by Design is aimed at overcoming barriers and moving toward open science as the default approach across the research enterprise. This report explores specific examples of open science and discusses a range of challenges, focusing on stakeholder perspectives. It is meant to provide guidance to the research enterprise and its stakeholders as they build strategies for achieving open science and take the next steps.},
  doi           = {10.17226/25116},
  owner         = {vilhuber},
  timestamp     = {2018.11.26},
  url           = {https://www.nap.edu/catalog/25116/open-science-by-design-realizing-a-vision-for-21st-century},
}

@Article{Duvendack2017,
  author    = {Maren Duvendack and Richard Palmer-Jones and W. Robert Reed},
  title     = {What Is Meant by {\textquotedblleft}Replication{\textquotedblright} and Why Does It Encounter Resistance in Economics?},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {46--51},
  month     = may,
  doi       = {10.1257/aer.p20171031},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Hamermesh2017,
  author    = {Daniel S. Hamermesh},
  title     = {Replication in Labor Economics: Evidence from Data and What It Suggests},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {37--40},
  month     = may,
  doi       = {10.1257/aer.p20171121},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Sukhtankar2017,
  author    = {Sandip Sukhtankar},
  title     = {Replications in Development Economics},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {32--36},
  month     = may,
  doi       = {10.1257/aer.p20171120},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Hoeffler2017,
  author    = {Jan H. Hoeffler},
  title     = {Replication and Economics Journal Policies},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {52--55},
  month     = may,
  doi       = {10.1257/aer.p20171032},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Coffman2017,
  author    = {Lucas C. Coffman and Muriel Niederle and Alistair J. Wilson},
  title     = {A Proposal to Organize and Promote Replications},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {41--45},
  month     = may,
  doi       = {10.1257/aer.p20171122},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Chang2017,
  author    = {Andrew C. Chang and Phillip Li},
  title     = {A Preanalysis Plan to Replicate Sixty Economics Research Papers That Worked Half of the Time},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {60--64},
  month     = may,
  doi       = {10.1257/aer.p20171034},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Berry2017,
  author    = {James Berry and Lucas C. Coffman and Douglas Hanley and Rania Gihleb and Alistair J. Wilson},
  title     = {Assessing the Rate of Replication in Economics},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {27--31},
  month     = may,
  doi       = {10.1257/aer.p20171119},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Article{Anderson2017,
  author    = {Richard G. Anderson and Areerat Kichkha},
  title     = {Replication, Meta-Analysis, and Research Synthesis in Economics},
  journal   = {American Economic Review},
  year      = {2017},
  volume    = {107},
  number    = {5},
  pages     = {56--59},
  month     = may,
  doi       = {10.1257/aer.p20171033},
  owner     = {vilhuber},
  publisher = {American Economic Association},
  timestamp = {2018.11.26},
}

@Misc{JacobyShouldJournalsBe2017,
  author        = {Jacoby, William G. and {Lafferty-Hess}, Sophia and Christian, Thu-Mai},
  title         = {Should {{Journals Be Responsible}} for {{Reproducibility}}?},
  month         = jul,
  year          = {2017},
  __markedentry = {[vilhuber:6]},
  abstract      = {One of the top journals in political science makes data-sharing and replication part of the publication process.},
  file          = {Snapshot:/home/vilhuber/Zotero/storage/8ZDX4TAS/should-journals-be-responsible-reproducibility.html:text/html},
  journal       = {Inside Higher Ed},
  language      = {en},
  owner         = {vilhuber},
  shorttitle    = {Should Journals Be Responsible for Reproducibility?},
  timestamp     = {2018-07-22T23:36:21Z},
  url           = {https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility},
  urldate       = {2018-07-22},
}

@Article{Clemens2015,
  author    = {Michael A. Clemens},
  title     = {THE MEANING OF FAILED REPLICATIONS: A REVIEW AND PROPOSAL},
  journal   = {Journal of Economic Surveys},
  year      = {2015},
  volume    = {31},
  number    = {1},
  pages     = {326--342},
  month     = dec,
  doi       = {10.1111/joes.12139},
  owner     = {vilhuber},
  publisher = {Wiley},
  timestamp = {2018.11.26},
}

@Article{Pesaran2003,
  author    = {Hashem Pesaran},
  title     = {Introducing a replication section},
  journal   = {Journal of Applied Econometrics},
  year      = {2003},
  volume    = {18},
  number    = {1},
  pages     = {111--111},
  month     = jan,
  doi       = {10.1002/jae.709},
  owner     = {vilhuber},
  publisher = {Wiley},
  timestamp = {2018.11.26},
}

@Article{Hoeffler2017a,
  author    = {Jan H. Höffler},
  title     = {{ReplicationWiki}: Improving Transparency in Social Sciences Research},
  journal   = {D-Lib Magazine},
  year      = {2017},
  volume    = {23},
  number    = {3/4},
  month     = mar,
  doi       = {10.1045/march2017-hoeffler},
  owner     = {vilhuber},
  publisher = {{CNRI} Acct},
  timestamp = {2018.11.26},
}

@Article{Hamermesh2007,
  author    = {Daniel S. Hamermesh},
  title     = {Viewpoint: Replication in economics},
  journal   = {Canadian Journal of Economics/Revue canadienne d{\textquotesingle}{\'{e}}conomique},
  year      = {2007},
  volume    = {40},
  number    = {3},
  pages     = {715--733},
  month     = jul,
  doi       = {10.1111/j.1365-2966.2007.00428.x},
  owner     = {vilhuber},
  publisher = {Wiley},
  timestamp = {2018.11.26},
}

@Article{Christian2018,
  author    = {Thu-Mai Lewis Christian and Sophia Lafferty-Hess and William G. Jacoby and Thomas M. Carsey},
  title     = {Operationalizing the Replication Standard: A Case Study of the Data Curation and Verification Workflow for Scholarly Journals},
  year      = {2018},
  month     = apr,
  doi       = {10.31235/osf.io/cfdba},
  owner     = {vilhuber},
  publisher = {Center for Open Science},
  timestamp = {2018.11.26},
}

@TechReport{MuellerLanger18,
  author={Frank Mueller-Langer and Benedikt Fecher and Dietmar Harhoff and Gert G. Wagner},
  title={{Replication Studies in Economics: How Many and Which Papers Are Chosen for Replication, and Why?}},
  year=2018,
  month=Apr,
  institution={Joint Research Centre (Seville site)},
  type={JRC Working Papers on Digital Economy},
  url={https://ideas.repec.org/p/ipt/decwpa/2018-01.html},
  number={2018-01},
  abstract={We investigate how often replication studies are published in empirical economics and what types of journal articles are eventually replicated. We find that from 1974 to 2014 0.10\% of publications in the Top 50 economics journals were replications. We take into account the results of replication (negating or reinforcing) and the extent of replication: narrow replication studies are typically devoted to mere replication of prior work while scientific replication studies provide a broader analysis. We find evidence that higher-impact articles and articles by authors from leading institutions are more likely to be subject to published replication studies whereas the probability of published replications is lower for articles that appeared in higher-ranked journals. Our analysis also suggests that mandatory data disclosure policies may have a positive effect on the incidence of replication.},
  keywords={Replication; economics of science; science policy; economic methodology},
  doi={},
}

@article{sukhtankar17,
  title={Replications in development economics},
  author={Sukhtankar, Sandip},
  journal={American Economic Review},
  volume={107},
  number={5},
  pages={32--36},
  year={2017}
}

@article{camerer2016,
  title={Evaluating replicability of laboratory experiments in economics},
  author={Camerer, Colin F and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and others},
  journal={Science},
  volume={351},
  number={6280},
  pages={1433--1436},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@Article{Christensen2018,
  author={Garret Christensen and Edward Miguel},
  title={{Transparency, Reproducibility, and the Credibility of Economics Research}},
  journal={Journal of Economic Literature},
  year=2018,
  volume={56},
  number={3},
  pages={920-980},
  month=sep,
  keywords={},
  doi={},
  abstract={There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  url={https://ideas.repec.org/a/aea/jeclit/v56y2018i3p920-80.html}
}

@article{Leamer1983,
  title={Let's take the con out of econometrics},
  author={Leamer, Edward E},
  journal={The American Economic Review},
  volume={73},
  number={1},
  pages={31--43},
  year={1983},
  publisher={JSTOR}
}

@article{Lalonde1986,
  title={Evaluating the econometric evaluations of training programs with experimental data},
  author={LaLonde, Robert J},
  journal={The American economic review},
  pages={604--620},
  year={1986},
  publisher={JSTOR}
}

@techreport{Gelman2013,
  title={The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
  author={Gelman, Andrew and Loken, Eric},
  year={2013},
  type = {Working Paper},
  institution={Department of Statistics, Columbia University},
  url={http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf}
}

@article{trisovic2021,
title={A large-scale study on research code quality and execution},
author={Trisovic, Ana and Lau, Matthew. K and Pasquier, Thomas and Crosas, Merce},
doi = {10.1038/s41597-022-01143-6},
url = {https://doi.org/10.1038/s41597-022-01143-6},
year={2022},
journal={Nature: Scientific Data},
volume={9},
number={60},
}


@Comment{jabref-meta: databaseType:bibtex;}

@techreport{perignon2022,
author = {Pérignon, Christophe and Akmansoy, Olivier and Hurlin, Christophe and Dreber, Anna and Holzmeister, Felix and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Menkveld, Albert and Razen, Michael and Weitzel, Utz},
year = {2022},
institution = {HEC Paris},
type = {Research Paper},
number = {FIN-2022-1467},
title = {Reproducibility of Empirical Results: Evidence from 1,000 Tests in Finance},
doi = {10.2139/ssrn.4064172},
url = {https://doi.org/10.2139/ssrn.4064172}
}

@techreport{Colliard2023,
  doi = {10.2139/ssrn.3418896},
  url = {https://doi.org/10.2139/ssrn.3418896},
  year = {2023},
  author = {Jean-Edourard Colliard and Christophe Hurlin and Christophe Perignon},
  title = {The Economics of Computational Reproducibility},
  institution = {HEC Paris},
  type = {Research Paper},
  number = {FIN-2019-1345}

}

@comment{our working papers}

@techreport{herbert2021,
author = {Herbert, Sylverie and Kingi, Hautahi and Stanchi, Flavio and Vilhuber, Lars}, 
title = {The Reproducibility of Economics Research: A Case Study},
year = 2021,
institution = {Banque de France},
type = {Working Paper},
url = {https://doi.org/10.2139/ssrn.4325149},
doi = {10.2139/ssrn.4325149}
}

@techreport{kingi2018-presentation,
author = {Kingi, Hautahi  and Vilhuber, Lars and Herbert, Sylverie and Stanchi, Flavio}, 
title = {Presentation: The Reproducibility of Economics Research: A Case Study},
year = 2018,
institution = {BITSS},
type = {presentation},
url = {https://hdl.handle.net/1813/60838}
}


@techreport{kingi2018,
author = {Kingi, Hautahi  and Vilhuber, Lars and Herbert, Sylverie and Stanchi, Flavio}, 
title = {The Reproducibility of Economics Research: A Case Study},
year = 2018,
institution = {BITSS},
type = {mimeo},
url = {https://osf.io/srg57}
}

@article{bernanke2004,
  title = {Editorial {{Statement}}},
  author = {Bernanke, Ben S.},
  year = {2004},
  journal = {The American Economic Review},
  volume = {94},
  number = {1},
  eprint = {3592790},
  eprinttype = {jstor},
  pages = {404--404},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/3592790},
  urldate = {2020-09-01},
  timestamp = {2020-09-01T16:05:33Z}
}

@Article{10.1257/app.2.3.228,
	doi = {10.1257/app.2.3.228},
	url = {https://doi.org/10.1257/app.2.3.228},
	year = {2010},
	month = jul,
	publisher = {American Economic Association},
	volume = {2},
	number = {3},
	pages = {228--255},
	author = {Marianne Bertrand and Claudia Goldin and Lawrence F Katz},
	title = {Dynamics of the Gender Gap for Young Professionals in the Financial and Corporate Sectors},
	journal = {American Economic Journal: Applied Economics},
}

@techreport{webofscience2018,
author = "Clarivate",
title = "Web of Science",
year = {2016-2018},
type = "Online resource",
url = "http://webofscience.com/",
note = "Accessed via Cornell University subscription"
}

@article{gleditsch2003,
  title={Posting your data: Will you be scooped or will you be famous},
  author={Gleditsch, Nils Petter and Metelits, Claire and Strand, Havard},
  journal={International Studies Perspectives},
  volume={4},
  number={1},
  pages={89--97},
  year={2003}
}

@article{perignon2019a,
	ids = {perignon2019d},
	title = {Certify Reproducibility with Confidential Data},
	author = {P{\'e}rignon, Christophe and Gadouche, Kamel and Hurlin, Christophe and Silberman, Roxane and Debonnel, Eric},
	year = {2019},
	month = jul,
	journal = {Science},
	volume = {365},
	number = {6449},
	pages = {127--128},
	publisher = {{American Association for the Advancement of Science}},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.aaw2825},
	url = {https://science.sciencemag.org/content/365/6449/127},
	urldate = {2019-09-22},
	abstract = {A trusted third party certifies that results reproduce A trusted third party certifies that results reproduce},
	copyright = {Copyright \textcopyright{} 2019, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	langid = {english},
	pmid = {31296759},
	timestamp = {2023-10-28T18:57:09Z},
	file = {Full Text PDF:/home/vilhuber/Zotero/storage/DJKGZEYS/Pérignon et al. - 2019 - Certify reproducibility with confidential data.pdf:application/pdf;Snapshot:/home/vilhuber/Zotero/storage/GQ9QZKZW/tab-article-info.html:text/html}
}

@techreport{greiner2023,
author = {Miloš Fišar and Ben Greiner and Christoph Huber and Elena Katok and Ali Ozkes and  {the Management Science Reproducibility Collaboration}},
title = {Reproducibility in Management Science}, 
type  = {Working Paper},
institution = {Wirtschaftsuniversität Wien},
year = 2023,
}

@article{10.1257/pandp.109.718,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2019},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {109},
  pages = {718--29},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.109.718},
  url = {http://www.aeaweb.org/articles?id=10.1257/pandp.109.718},
  urldate = {2019-09-21},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2021-02-06T16:57:48Z},
  file = {Full Text:/Users/larsvilhuber/Zotero/storage/587KRG5B/2019 - Report by the AEA Data Editor.pdf:application/pdf}
}

@article{American_Economic_Association2008-wayback,
  title = {Data Availability Policy},
  author = {{American Economic Association}},
  year = {2008},
  url = {https://web.archive.org/web/20180927113622/https://www.aeaweb.org/journals/policies/data-availability-policy},
  urldate = {2019-09-21},
  nonote = {(accessed: 2018-09-27 via Archive.org)},
  timestamp = {2019-12-22T21:40:29Z}
}

@article{ankel-peters2023,
  title = {Do Economists Replicate?},
  author = {{Ankel-Peters}, J{\"o}rg and Fiala, Nathan and Neubauer, Florian},
  year = {2023},
  month = aug,
  journal = {Journal of Economic Behavior \& Organization},
  volume = {212},
  pages = {219--232},
  issn = {0167-2681},
  doi = {10.1016/j.jebo.2023.05.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0167268123001555},
  urldate = {2023-10-09},
  abstract = {Reanalyses of empirical studies and replications in new contexts are important for scientific progress. Journals in economics increasingly require authors to provide data and code alongside published papers, but how much does the economics profession actually replicate? This paper summarizes existing replication definitions and reviews how much economists replicate other scholars' work. We argue that in order to counter incentive problems potentially leading to a replication crisis, replications in the spirit of Merton's `organized skepticism' are needed \textendash{} what we call `policing replications'. We review leading economics journals to show that policing replications are rare and conclude that more incentives to replicate are needed to reap the fruits of rising transparency standards.},
  keywords = {Generalizability,Meta-science,Replicability,Replication,Research transparency,Systematic review},
  timestamp = {2023-10-09T15:45:05Z},
  file = {ScienceDirect Snapshot:/Users/larsvilhuber/Zotero/storage/ITB4JPJZ/S0167268123001555.html:text/html}
}

@article{brodeur2023,
  title = {Replication Games: How to Make Reproducibility Research More Systematic},
  shorttitle = {Replication Games},
  author = {Brodeur, Abel and Dreber, Anna and {Hoces de la Guardia}, Fernando and Miguel, Edward},
  year = {2023},
  month = sep,
  journal = {Nature},
  volume = {621},
  number = {7980},
  pages = {684--686},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-023-02997-5},
  url = {https://www.nature.com/articles/d41586-023-02997-5},
  urldate = {2023-10-09},
  abstract = {In some areas of social science, around half of studies can't be replicated. A new test-fast, fail-fast initiative aims to show what research is hot \textemdash{} and what's not.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Economics,Research data,Research management,Scientific community,Sociology},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Research data, Research management, Scientific community, Economics, Sociology},
  timestamp = {2023-10-09T15:40:42Z},
  file = {Brodeur et al_2023_Replication games.pdf:/Users/larsvilhuber/Zotero/storage/EAXGH43Q/Brodeur et al_2023_Replication games.pdf:application/pdf}
}

@article{camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  url = {https://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2023-04-24},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Economics,Psychology},
  timestamp = {2023-04-24T00:53:58Z},
  file = {Camerer et al_2018_Evaluating the replicability of social science experiments in Nature and.pdf:/Users/larsvilhuber/Zotero/storage/CMGS3NRK/Camerer et al_2018_Evaluating the replicability of social science experiments in Nature and.pdf:application/pdf}
}

@article{cruwell2023,
  title = {What's in a {{Badge}}? {{A Computational Reproducibility Investigation}} of the {{Open Data Badge Policy}} in {{One Issue}} of {{Psychological Science}}},
  shorttitle = {What's in a {{Badge}}?},
  author = {Cr{\"u}well, Sophia and Apthorp, Deborah and Baker, Bradley J. and Colling, Lincoln and Elson, Malte and Geiger, Sandra J. and Lobentanzer, Sebastian and Mon{\'e}ger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas J. L.},
  year = {2023},
  month = apr,
  journal = {Psychological Science},
  volume = {34},
  number = {4},
  pages = {512--522},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/09567976221140828},
  url = {https://doi.org/10.1177/09567976221140828},
  urldate = {2023-10-19},
  abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
  langid = {english},
  timestamp = {2023-10-19T21:55:20Z},
  file = {Crüwell et al_2023_What’s in a Badge.pdf:/Users/larsvilhuber/Zotero/storage/ADQZW3A2/Crüwell et al_2023_What’s in a Badge.pdf:application/pdf}
}

@article{Glandon2011-ec,
  title = {Report on the {{American Economic Review Data Availability Compliance Project}}},
  author = {Glandon, Philip},
  year = {2011},
  journal = {Appendix to American Economic Review Editors Report},
  url = {https://web.archive.org/web/20130202231024/http://www.aeaweb.org/aer/2011_Data_Compliance_Report.pdf},
  timestamp = {2023-10-10T19:41:50Z},
  file = {Glandon_2011_Report on the American Economic Review Data Availability Compliance Project.pdf:/Users/larsvilhuber/Zotero/storage/SMDPNZ4H/Glandon_2011_Report on the American Economic Review Data Availability Compliance Project.pdf:application/pdf}
}

@article{HamermeshAm.Econ.Rev.2017,
  title = {Replication in {{Labor Economics}}: {{Evidence}} from {{Data}}, and {{What It Suggests}}},
  shorttitle = {Replication in {{Labor Economics}}},
  author = {Hamermesh, Daniel S.},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {37--40},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171121},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171121},
  urldate = {2018-05-22},
  abstract = {Examining the most heavily cited publications in labor economics from the early 1990s, I show that few of over 3,000 articles, citing them directly, replicates them. They are replicated more frequently using data from other time periods and economies, so that the validity of their central ideas has typically been verified. This pattern of scholarship suggests, beyond the currently required depositing of data and code upon publication, that there is little need for formal mechanisms for replication. The market for scholarship already produces replications of non-laboratory applied research.},
  langid = {english},
  timestamp = {2019-12-22T22:00:38Z},
  file = {Hamermesh_2017_Replication in Labor Economics.pdf:/Users/larsvilhuber/Zotero/storage/G35HY7W8/Hamermesh_2017_Replication in Labor Economics.pdf:application/pdf;Snapshot:/Users/larsvilhuber/Zotero/storage/RVD3DWI2/articles.html:text/html}
}

@article{huntington-klein2021,
  title = {The Influence of Hidden Researcher Decisions in Applied Microeconomics},
  author = {{Huntington-Klein}, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey R. and Burli, Pralhad and Chen, Naibin and Grieco, Paul and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin and Stopnitzky, Yaniv},
  year = {2021},
  journal = {Economic Inquiry},
  volume = {59},
  number = {3},
  pages = {944--960},
  issn = {1465-7295},
  doi = {10.1111/ecin.12992},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ecin.12992},
  urldate = {2021-09-27},
  abstract = {Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions. Two published causal empirical results are replicated by seven replicators each. We find large differences in data preparation and analysis decisions, many of which would not likely be reported in a publication. No two replicators reported the same sample size. Statistical significance varied across replications, and for one of the studies the effect's sign varied as well. The standard deviation of estimates across replications was 3\textendash 4 times the mean reported standard error.},
  langid = {english},
  keywords = {metascience,replication,research},
  timestamp = {2021-09-27T21:16:29Z},
  file = {Huntington-Klein et al_2021_The influence of hidden researcher decisions in applied microeconomics.pdf:/Users/larsvilhuber/Zotero/storage/3L262JHS/Huntington-Klein et al_2021_The influence of hidden researcher decisions in applied microeconomics.pdf:application/pdf;Snapshot:/Users/larsvilhuber/Zotero/storage/RP47YDY6/ecin.html:text/html}
}

@article{mas2019,
  title = {Report of the {{Editor}}: {{American Economic Journal}}: {{Applied Economics}}},
  shorttitle = {Report of the {{Editor}}},
  author = {Mas, Alex},
  year = {2019},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {109},
  pages = {639--645},
  issn = {2574-0768},
  doi = {10.1257/pandp.109.639},
  url = {https://www.aeaweb.org/articles?id=10.1257/pandp.109.639&ArticleSearch%5Bwithin%5D%5Barticletitle%5D=1&ArticleSearch%5Bwithin%5D%5Barticleabstract%5D=1&ArticleSearch%5Bwithin%5D%5Bauthorlast%5D=1&ArticleSearch%5Bq%5D=report&JelClass%5Bvalue%5D=0},
  urldate = {2023-10-08},
  langid = {english},
  timestamp = {2023-10-08T20:10:34Z},
  file = {Mas_2019_Report of the Editor.pdf:/Users/larsvilhuber/Zotero/storage/YL9ZC45C/Mas_2019_Report of the Editor.pdf:application/pdf}
}

@misc{menkveld2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Non-{{Standard Errors}}},
  author = {Menkveld, Albert J. and Dreber, Anna and Holzmeister, Felix and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Razen, Michael and Weitzel, Utz and Abad, David and Abudy, Menachem (Meni) and Adrian, Tobias and {Ait-Sahalia}, Yacine and Akmansoy, Olivier and Alcock, Jamie and Alexeev, Vitali and Aloosh, Arash and Amato, Livia and Amaya, Diego and Angel, James and Bach, Amadeus and Baidoo, Edwin and Bakalli, Gaetan and Barbon, Andrea and Bashchenko, Oksana and Bindra, Parampreet Christopher and Bjonnes, Geir Hoidal and Black, Jeff and Black, Bernard S. and Bohorquez, Santiago and Bondarenko, Oleg and Bos, Charles S. and {Bosch-Rosa}, Ciril and Bouri, Elie and Brownlees, Christian T. and Calamia, Anna and Cao, Viet Nga and {Capelle-Blancard}, Gunther and Capera, Laura and Caporin, Massimiliano and Carrion, Allen and Caskurlu, Tolga and Chakrabarty, Bidisha and Chernov, Mikhail and Cheung, William M. and Chincarini, Ludwig B. and Chordia, Tarun and Chow, Sheung Chi and Clapham, Benjamin and Colliard, Jean-Edouard and {Comerton-Forde}, Carole and Curran, Edward and Dao, Thong and Dare, Wale and Davies, Ryan J. and De Blasis, Riccardo and De Nard, Gianluca and Declerck, Fany and Deev, Oleg and Degryse, Hans and Deku, Solomon and Desagre, Christophe and {van Dijk}, Mathijs A. and Dim, Chukwuma and Dimpfl, Thomas and Dong, Yunjiang and Drummond, Philip and Dudda, Tom L. and Dumitrescu, Ariadna and Dyakov, Teodor and Dyhrberg, Anne Haubo and Dzieli{\'n}ski, Micha{\l} and Eksi, Asli and El Kalak, Izidin and {ter Ellen}, Saskia and Eugster, Nicolas and Evans, Martin D. D. and Farrell, Michael and {F{\'e}lez-Vi{\~n}as}, Ester and Ferrara, Gerardo and Ferrouhi, El Mehdi and Flori, Andrea and {Fluharty-Jaidee}, Jonathan and Foley, Sean and Fong, Kingsley Y. L. and Foucault, Thierry and Franus, Tatiana and Franzoni, Francesco A. and Frijns, Bart and Fr{\"o}mmel, Michael and Fu, Servanna and F{\"u}llbrunn, Sascha and Gan, Baoqing and Gehrig, Thomas and Gerritsen, Dirk and {Gil-Bazo}, Javier and Glosten, Lawrence R. and Gomez, Thomas and Gorbenko, Arseny and G{\"u}{\c c}bilmez, Ufuk and Grammig, Joachim and Gregoire, Vincent and Hagstr{\"o}mer, Bj{\"o}rn and Hambuckers, Julien and Hapnes, Erik and Harris, Jeffrey H. and Harris, Lawrence and Hartmann, Simon and Hasse, Jean-Baptiste and Hautsch, Nikolaus and He, Xuezhong and Heath, Davidson and Hediger, Simon and Hendershott, Terrence and Hibbert, Ann Marie and Hjalmarsson, Erik and Hoelscher, Seth A. and Hoffmann, Peter and Holden, Craig W. and Horenstein, Alex R. and Huang, Wenqian and Huang, Da and Hurlin, Christophe and Ivashchenko, Alexey and Iyer, Subramanian R. and Jahanshahloo, Hossein and Jalkh, Naji and Jones, Charles M. and Jurkatis, Simon and Jylha, Petri and Kaeck, Andreas and Kaiser, Gabriel and Karam, Arz{\'e} and Karmaziene, Egle and Kassner, Bernhard and Kaustia, Markku and Kazak, Ekaterina and Kearney, Fearghal and {van Kervel}, Vincent and Khan, Saad and Khomyn, Marta and Klein, Tony and Klein, Olga and Klos, Alexander and Koetter, Michael and Krahnen, Jan Pieter and Kolokolov, Aleksey and Korajczyk, Robert A. and Kozhan, Roman and Kwan, Amy and Lajaunie, Quentin and Lam, FY Eric and Lambert, Marie and Langlois, Hugues and Lausen, Jens and Lauter, Tobias and Leippold, Markus and Levin, Vladimir and Li, Yijie and Li, (Michael) Hui and Liew, Chee Yoong and Lindner, Thomas and Linton, Oliver B. and Liu, Jiacheng and Liu, Anqi and Llorente, Guillermo and Lof, Matthijs and Lohr, Ariel and Longstaff, Francis A. and {Lopez-Lira}, Alejandro and Mankad, Shawn and Mano, Nicola and Marchal, Alexis and Martineau, Charles and Mazzola, Francesco and Meloso, Debrah and Mihet, Roxana and Mohan, Vijay and Moinas, Sophie and Moore, David and Mu, Liangyi and Muravyev, Dmitriy and Murphy, Dermot and Neszveda, Gabor and Neumeier, Christian and Nielsson, Ulf and Nimalendran, Mahendrarajah and Nolte, Sven and Norden, Lars L. and O'Neill, Peter and Obaid, Khaled and {\O}degaard, Bernt Arne and {\"O}stberg, Per and Painter, Marcus and Palan, Stefan and Palit, Imon and Park, Andreas and Pascual, Roberto and Pasquariello, Paolo and Pastor, Lubos and Patel, Vinay and Patton, Andrew J. and Pearson, Neil D. and Pelizzon, Loriana and Pelster, Matthias and P{\'e}rignon, Christophe and Pfiffer, Cameron and Philip, Richard and Pl{\'i}hal, Tom{\'a}{\v s} and Prakash, Puneet and Press, Oliver-Alexander and Prodromou, Tina and Putni{\c n}{\v s}, T{\=a}lis J. and Raizada, Gaurav and Rakowski, David A. and Ranaldo, Angelo and Regis, Luca and Reitz, Stefan and Renault, Thomas and Renjie, Rex Wang and Ren{\`o}, Roberto and Riddiough, Steven and Rinne, Kalle and Rintam{\"a}ki, Paul and Riordan, Ryan and Rittmannsberger, Thomas and {Rodr{\'i}guez-Longarela}, I{\~n}aki and R{\"o}sch, Dominik and Rognone, Lavinia and Roseman, Brian and Rosu, Ioanid and Roy, Saurabh and Rudolf, Nicolas and Rush, Stephen and Rzayev, Khaladdin and Rze{\'z}nik, Aleksandra and Sanford, Anthony and Sankaran, Harikumar and Sarkar, Asani and Sarno, Lucio and Scaillet, O. and Scharnowski, Stefan and {Schenk-Hopp{\'e}}, Klaus Reiner and Schertler, Andrea and Schneider, Michael and Schroeder, Florian and Schuerhoff, Norman and Schuster, Philipp and Schwarz, Marco A. and Seasholes, Mark S. and Seeger, Norman and Shachar, Or and Shkilko, Andriy and Shui, Jessica and Sikic, Mario and Simion, Giorgia and Smales, Lee A. and S{\"o}derlind, Paul and Sojli, Elvira and Sokolov, Konstantin and Spokeviciute, Laima and Stefanova, Denitsa and Subrahmanyam, Marti G. and Neus{\"u}ss, Sebastian and Szaszi, Barnabas and Talavera, Oleksandr and Tang, Yuehua and Taylor, Nicholas and Tham, Wing Wah and Theissen, Erik and Thimme, Julian and Tonks, Ian and Tran, Hai and Trapin, Luca and Trolle, Anders B. and Valente, Giorgio and Van Ness, Robert A. and Vasquez, Aurelio and Verousis, Thanos and Verwijmeren, Patrick and Vilhelmsson, Anders and Vilkov, Grigory and Vladimirov, Vladimir and Vogel, Sebastian and Voigt, Stefan and Wagner, Wolf and Walther, Thomas and Weiss, Patrick and {van der Wel}, Michel and Werner, Ingrid M. and Westerholm, P. Joakim and Westheide, Christian and Wipplinger, Evert and Wolf, Michael and Wolff, Christian C. P. and Wolk, Leonard and Wong, Wing-Keung and Wrampelmeyer, Jan and Xia, Shuo and Xiu, Dacheng and Xu, Ke and Xu, Caihong and Yadav, Pradeep K. and Yag{\"u}e, Jos{\'e} and Yan, Cheng and Yang, Antti and Yoo, Woongsun and Yu, Wenjia and Yu, Shihao and Yueshen, Bart Zhou and Yuferova, Darya and Zamojski, Marcin and Zareei, Abalfazl and Zeisberger, Stefan and Zhang, S. Sarah and Zhang, Xiaoyu and Zhong, Zhuo and Zhou, Z. Ivy and Zhou, Chen and Zhu, Xingyu Sonya and Zoican, Marius and Zwinkels, Remco C. J. and Chen, Jian and Duevski, Teodor and Gao, Ge and Gemayel, Roland and Gilder, Dudley and Kuhle, Paul and Pagnotta, Emiliano and Pelli, Michele and S{\"o}nksen, Jantje and Zhang, Lu and Ilczuk, Konrad and Bogoev, Dimitar and Qian, Ya and Wika, Hans C. and Yu, Yihe and Zhao, Lu and Mi, Michael and Bao, Li and Vaduva, Andreea and Prokopczuk, Marcel and Avetikian, Alejandro and Wu, Zhen-Xing},
  year = {2023},
  month = may,
  number = {3961574},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.3961574},
  url = {https://papers.ssrn.com/abstract=3961574},
  urldate = {2023-10-09},
  abstract = {In statistics, samples are drawn from a population in a data-generating process (DGP). Standard errors measure the uncertainty in estimates of population parameters. In science, evidence is generated to test hypotheses in an evidence-generating process (EGP). We claim that EGP variation across researchers adds uncertainty: Non-standard errors (NSEs). We study NSEs by letting 164 teams test the same hypotheses on the same data. NSEs turn out to be sizable, but smaller for better reproducible or higher rated research. Adding peer-review stages reduces NSEs. We further find that this type of uncertainty is underestimated by participants.},
  langid = {english},
  keywords = {liquidity,multi-analyst approach,non-standard errors},
  timestamp = {2023-10-09T03:12:18Z},
  file = {Menkveld et al_2023_Non-Standard Errors.pdf:/Users/larsvilhuber/Zotero/storage/A24XSCZQ/Menkveld et al_2023_Non-Standard Errors.pdf:application/pdf}
}

@misc{nagel2018,
  title = {Code-Sharing Policy: Update},
  shorttitle = {Code-Sharing Policy},
  author = {Nagel, Stefan},
  year = {2018},
  month = mar,
  journal = {Journal of Finance},
  url = {https://voices.uchicago.edu/jfeditor/2018/03/06/code-sharing-policy-update/},
  urldate = {2023-10-09},
  abstract = {In September 2016, the JF adopted a code-sharing policy. The first accepted papers with replication code are now online (scroll down and click on supporting information): Reproducibility of researc\ldots},
  langid = {american},
  timestamp = {2023-10-09T15:09:09Z},
  file = {Snapshot:/Users/larsvilhuber/Zotero/storage/2S8WBMLK/code-sharing-policy-update.html:text/html}
}

@book{nasem2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2019},
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/25303},
  url = {https://doi.org/10.17226/25303},
  urldate = {2019-09-21},
  isbn = {978-0-309-48616-3},
  timestamp = {2023-10-08T16:25:08Z}
}

@article{obels2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Dani{\"e}l and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {229--237},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  url = {https://doi.org/10.1177/2515245920918872},
  urldate = {2023-10-19},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english},
  timestamp = {2023-10-19T21:55:07Z},
  file = {Obels et al_2020_Analysis of Open Data and Computational Reproducibility in Registered Reports.pdf:/Users/larsvilhuber/Zotero/storage/QNTN87C8/Obels et al_2020_Analysis of Open Data and Computational Reproducibility in Registered Reports.pdf:application/pdf}
}

@article{openalex2022,
  title = {{{OpenAlex}}: {{A}} Fully-Open Index of Scholarly Works, Authors, Venues, Institutions, and Concepts},
  shorttitle = {{{OpenAlex}}},
  author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01833},
  eprint = {2205.01833},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.01833},
  url = {http://arxiv.org/abs/2205.01833},
  urldate = {2023-10-12},
  abstract = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Digital Libraries},
  timestamp = {2023-10-12T14:02:20Z},
  file = {Priem et al_2022_OpenAlex.pdf:/Users/larsvilhuber/Zotero/storage/8C3GIA7Y/Priem et al_2022_OpenAlex.pdf:application/pdf;arXiv.org Snapshot:/Users/larsvilhuber/Zotero/storage/8WXGRYL5/2205.html:text/html}
}

@article{opensciencecollaboration2015b,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {https://www.science.org/doi/10.1126/science.aac4716},
  urldate = {2023-04-24},
  abstract = {Empirically analyzing empirical evidence                            One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts               et al.               describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.                                         Science               , this issue               10.1126/science.aac4716                        ,              A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.           ,                             INTRODUCTION               Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.                                         RATIONALE               There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.                                         RESULTS                                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and                 P                 values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (                 M                 r                 = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (                 M                 r                 = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (                 P                 {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.                                                        CONCLUSION                                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original                 P                 value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.                              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.                                                   Original study effect size versus replication effect size (correlation coefficients).                   Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.                                                                         ,              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  timestamp = {2023-04-24T01:02:27Z},
  file = {Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf:/Users/larsvilhuber/Zotero/storage/M9QIGGBL/Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf:application/pdf}
}

@misc{ourresearch2023,
  title = {{{OpenAlex}}},
  author = {{OurResearch}},
  year = {2023},
  url = {https://openalex.org/about},
  urldate = {2023-10-12},
  timestamp = {2023-10-12T14:01:59Z}
}

@misc{perignon2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Computational {{Reproducibility}} in {{Finance}}: {{Evidence}} from 1,000 {{Tests}}},
  shorttitle = {Computational {{Reproducibility}} in {{Finance}}},
  author = {P{\'e}rignon, Christophe and Akmansoy, Olivier and Hurlin, Christophe and Dreber, Anna and Holzmeister, Felix and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Menkveld, Albert J. and Razen, Michael and Weitzel, Utz},
  year = {2023},
  month = jul,
  number = {4064172},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.4064172},
  url = {https://papers.ssrn.com/abstract=4064172},
  urldate = {2023-09-27},
  abstract = {We analyze the computational reproducibility of more than 1,000 empirical answers to six research questions in finance provided by 168 international research teams. Running the original researchers' code on the same raw data regenerates exactly the same results only 52\% of the time. Reproducibility is higher for researchers with better coding skills and for those exerting more effort. It is lower for more technical research questions, more complex code, and for results lying in the tails of the results distribution. Neither researcher seniority, nor peer-review ratings appear to be related to the level of reproducibility. Moreover, researchers exhibit strong overconfidence when assessing the reproducibility of their own research. We provide guidelines for finance researchers and discuss several implementable reproducibility policies for academic journals.},
  langid = {english},
  keywords = {computational reproducibility,credibility of research,data-availability policy,multi-analyst study,open science,scientific publishing},
  timestamp = {2023-09-27T20:06:58Z},
  file = {Pérignon et al_2023_Computational Reproducibility in Finance.pdf:/Users/larsvilhuber/Zotero/storage/Z69Q6FKW/Pérignon et al_2023_Computational Reproducibility in Finance.pdf:application/pdf}
}

@article{serra-garcia2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  journal = {Science Advances},
  volume = {7},
  number = {21},
  pages = {eabd1705},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  url = {https://advances.sciencemag.org/content/7/21/eabd1705},
  urldate = {2021-05-22},
  abstract = {We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility. Published papers that fail to replicate are cited more than those that replicate, even after the failure is published. Published papers that fail to replicate are cited more than those that replicate, even after the failure is published.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. https://creativecommons.org/licenses/by-nc/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  timestamp = {2021-05-22T01:50:48Z},
  file = {Serra-Garcia_Gneezy_2021_Nonreplicable publications are cited more than replicable ones.pdf:/Users/larsvilhuber/Zotero/storage/AZCIS2ER/Serra-Garcia_Gneezy_2021_Nonreplicable publications are cited more than replicable ones.pdf:application/pdf;Snapshot:/Users/larsvilhuber/Zotero/storage/ZTM9ZAYD/eabd1705.html:text/html}
}

@article{templateREADMEv1.0,
  title = {A Template {{README}} for Social Science Replication Packages},
  author = {Vilhuber, Lars and Connolly, Marie and Koren, Mikl{\'o}s and Llull, Joan and Morrow, Peter},
  year = {2020},
  month = dec,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.4319999},
  url = {https://zenodo.org/record/4319999},
  urldate = {2021-04-01},
  abstract = {The typical README in social science journals serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper, including the description of the origins of data and/or description of programs. As such, a good README file should first provide a brief overview of the available material and a brief guide as to how to proceed from beginning to end, before then diving into the specifics. These template files structure such a README in a way that is compliant with the typical data and code workflow in the social sciences.},
  copyright = {Creative Commons Attribution Non Commercial 4.0 International, Open Access},
  langid = {english},
  keywords = {economics,reproducibility,social sciences},
  timestamp = {2023-10-10T21:34:59Z}
}

@techreport{templateREADMEv1.1,
  title = {A Template {{README}} for Social Science Replication Packages},
  author = {Vilhuber, Lars and Connolly, Marie and Koren, Mikl{\'o}s and Llull, Joan and Morrow, Peter},
  year = {2022},
  month = nov,
  number = {v1.1.0},
  institution = {{Zenodo}},
  url = {https://doi.org/10.5281/zenodo.7293838},
  urldate = {2023-05-17},
  abstract = {The typical README in social science journals serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper, including the description of the origins of data and/or description of programs. As such, a good README file should first provide a brief overview of the available material and a brief guide as to how to proceed from beginning to end, before then diving into the specifics. These template files structure such a README in a way that is compliant with the typical data and code workflow in the social sciences.},
  keywords = {economics,reproducibility,social sciences},
  timestamp = {2023-10-10T21:34:20Z},
  file = {Zenodo Snapshot:/Users/larsvilhuber/Zotero/storage/RX257YKW/7293838.html:text/html}
}

@article{vilhuber2020a,
  title = {Reproducibility and {{Replicability}} in {{Economics}}},
  author = {Vilhuber, Lars},
  year = {2020},
  month = dec,
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {4},
  doi = {10.1162/99608f92.4f6b9e67},
  url = {https://hdsr.mitpress.mit.edu/pub/fgpmpj1l},
  copyright = {All rights reserved},
  timestamp = {2023-10-10T21:35:41Z}
}

@article{vilhuber2021,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2021},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {111},
  pages = {808--817},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.111.808},
  url = {https://pubs.aeaweb.org/doi/10.1257/pandp.111.808},
  urldate = {2021-05-20},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2021-05-20T14:42:52Z},
  file = {2021_Report by the AEA Data Editor.pdf:/Users/larsvilhuber/Zotero/storage/TL7UKNWC/2021_Report by the AEA Data Editor.pdf:application/pdf}
}

@article{vilhuber2022a,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2022},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {112},
  pages = {813--23},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.112.813},
  langid = {english},
  timestamp = {2022-06-01T15:52:55Z}
}

@article{vilhuber2022b,
  ids = {vilhuber2022f},
  title = {Teaching for Large-Scale {{Reproducibility Verification}}},
  author = {Vilhuber, Lars and Son, Hyuk Harry and Welch, Meredith and Wasser, David N. and Darisse, Michael},
  year = {2022},
  month = sep,
  journal = {Journal of Statistics and Data Science Education},
  volume = {30},
  number = {3},
  pages = {274--281},
  doi = {10.1080/26939169.2022.2074582},
  url = {https://arxiv.org/abs/2204.01540v1},
  abstract = {We describe a unique environment in which undergraduate students from various STEM and social science disciplines are trained in data provenance and reproducible methods, and then apply that knowledge to real, conditionally accepted manuscripts and associated replication packages. We describe in detail the recruitment, training, and regular activities. While the activity is not part of a regular curriculum, the skills and knowledge taught through explicit training of reproducible methods and principles, and reinforced through repeated application in a real-life workflow, contribute to the education of these undergraduate students, and prepare them for post-graduation jobs and further studies.},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2022-12-07T02:46:40Z},
  file = {Vilhuber et al_2022_Teaching for Large-Scale Reproducibility Verification.pdf:/Users/larsvilhuber/Zotero/storage/IKKSYQFN/Vilhuber et al_2022_Teaching for Large-Scale Reproducibility Verification.pdf:application/pdf}
}

@article{VilhuberAEAPap.Proc.2020,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars and Turitto, James and Welch, Keesler},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {764--75},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.110.764},
  copyright = {All rights reserved},
  langid = {english},
  timestamp = {2020-11-21T21:59:57Z}
}

@article{vlaeminck2021,
  title = {Dawning of a New Age? {{Economics}} Journals' Data Policies on the Test Bench},
  shorttitle = {Dawning of a New Age?},
  author = {Vlaeminck, Sven},
  year = {2021},
  month = aug,
  journal = {LIBER Quarterly: The Journal of the Association of European Research Libraries},
  volume = {31},
  number = {1},
  pages = {1--29},
  issn = {2213-056X},
  doi = {10.53377/lq.10940},
  url = {https://liberquarterly.eu/article/view/10940},
  urldate = {2021-08-31},
  abstract = {In the field of social sciences and particularly in economics, studies have frequently reported a lack of reproducibility of published research. Most often, this is due to the unavailability of data reproducing the findings of a study. However, over the past years, debates on open science practices and reproducible research have become stronger and louder among research funders, learned societies, and research organisations. Many of these have started to implement data policies to overcome these shortcomings. Against this background, the article asks if there have been changes in the way economics journals handle data and other materials that are crucial to reproduce the findings of empirical articles. For this purpose, all journals listed in the Clarivate Analytics Journal Citation Reports edition for economics have been evaluated for policies on the disclosure of research data. The article describes the characteristics of these data policies and explicates their requirements. Moreover, it compares the current findings with the situation some years ago. The results show significant changes in the way journals handle data in the publication process. Research libraries can use the findings of this study for their advisory activities to best support researchers in submitting and providing data as required by journals.},
  copyright = {Copyright (c) 2021 Sven Vlaeminck},
  langid = {english},
  keywords = {data policies,economics,journals,open science,reproducibility},
  timestamp = {2021-08-31T17:27:40Z}
}

@inproceedings{wang2020,
  title = {Assessing and Restoring Reproducibility of {{Jupyter}} Notebooks},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller, Andreas},
  year = {2020},
  month = dec,
  pages = {138--149},
  publisher = {{ACM}},
  address = {{Virtual Event Australia}},
  doi = {10.1145/3324884.3416585},
  url = {https://dl.acm.org/doi/10.1145/3324884.3416585},
  urldate = {2023-03-22},
  isbn = {978-1-4503-6768-4},
  langid = {english},
  timestamp = {2023-03-22T18:54:05Z},
  file = {Wang et al_2020_Assessing and restoring reproducibility of Jupyter notebooks.pdf:/Users/larsvilhuber/Zotero/storage/MGUTI7A5/Wang et al_2020_Assessing and restoring reproducibility of Jupyter notebooks.pdf:application/pdf}
}

@article{welch2019c,
  title = {Reproducing, {{Extending}}, {{Updating}},{{Replicating}}, {{Reexamining}}, and {{Reconciling}}},
  author = {Welch, Ivo},
  year = {2019},
  month = dec,
  journal = {Critical Finance Review},
  volume = {8},
  number = {1-2},
  pages = {301--304},
  issn = {21645744, 21645760},
  doi = {10.1561/104.00000082},
  url = {http://www.nowpublishers.com/article/Details/CFR-0082},
  urldate = {2023-10-09},
  annotation = {https://cfr.pub/published/papers/welch2020reproducing.pdf},
  timestamp = {2023-10-09T15:05:34Z}
}
