Replication, reproduction, and falsification of published articles are important steps in the scientific process. These activities help to make science ``robust and reliable'' \parencite{Bollen2015-vb} and are a \textit{sine qua non} condition for the credibility of economic research. Robust and replicable research is especially important in policy institutions such as central banks or governments since it provides input needed for its core activities and informs their decisions. Given its importance for both research and policy purposes, the reproducibility and replicability\footnote{For precise definitions of these terms, see \parencite{Bollen2015-vb,nasem2019} and definitions in the next section.} of articles has been discussed in economics for at least thirty years\footnote{\textcite{Dewald1986,Vinod2005,AndersonEtAl2005,King95,BurmanEtAl2010,Duvendack2017,Hamermesh2017,Sukhtankar2017,Hoeffler2017,Coffman2017,Chang2017,Berry2017,Anderson2017}}. Since the early 2000s, many economics journals have a data availability policy requiring authors to deposit materials sufficient to reproduce the analysis in their article. The goal is twofold: the exercise of compiling the replication package itself provides an opportunity for authors to fix errors on their own. And the availability of these materials is meant to reduce the cost of replication and as a ``starting point for other researchers'' \citep{bernanke2004}. However, compliance is typically only lightly monitored (some authors have referred to it as the ``honors system''), and no pre-publication verification is conducted. In this paper, we carry out a large reproducibility exercise of published articles in a journal which implements such data availability policy.

Actual published reproductions or replications are rare \parencite{BellMiller2013b,Duvendack2017}, notwithstanding some recent examples \parencite{Hoeffler2017a,Chang2017,ChangLi2015,camerer2016}. For example, \textcite{MuellerLanger18} found that just 0.1\% of the 126,505 articles published between 1974 and 2014 in the top 50 economics journals were replications. \textcite{sukhtankar17} found that, of the 1,138 empirical development economics articles published between 2000 and 2015 in the ``top 10" economics journals,\footnote{\textcite{sukhtankar17} combines the traditional ``top 5" with the \ac{AEJ:AE}, the \ac{AEJ:EP}, the \ac{EJ}, the \ac{JEEA} and the \ac{ReStat}.} just 6.2\% were replicated in a published or working paper. The paucity of replications in economics is, in part, because it is often difficult to find the materials required to conduct reproducibility or replication exercises \parencite{Dewald1986,McCullough2006,McCullough03}. Despite a long standing explicit recognition of the importance of replication in economics \parencite{Frisch1933}, it has been suggested that ``there is no tradition of replication in economics" \parencite[p. 1093]{McCullough2006}.\footnote{Though \textcite{Hamermesh2007,Hamermesh2017} disagrees.} An exception has been the Journal of Applied Econometrics (since 2003) and more recently the newly created \ac{JPE} Micro, announced in February 2022.
 % This sentence may be too general. The scientific community is actively engaged in identifying ways to support replication \parencite{Bollen2015-vb,NAP25116}.}

To promote transparency, journals have adopted ``data (and code) availability'' policies (DAP). The \ac{AEA} announced one in 2004 \parencite{bernanke2004}, the \ac{JPE} in 2004, the \ac{QJE} was one of the last major economics journals to do so in 2016. Data availability policies require authors to make all data (and generally code) necessary to reproduce their study available, or when legal restriction prohibit the sharing of the data, authors should explain how to get access to the data. As of 2016, the top five economics journals all have a DAP. However, it is still not the norm among economics journals. Studies find that between 57\% and 66\% of economics journals (with some empirical content) have a DAP that requires or suggests deposit of materials, and between 12 and 14\% let authors provide materials on request \parencite{Hoeffler2017,vlaeminck2021}. There are also differences in the coerciveness of the DAP across journals, with only a small minority making deposit of materials mandatory\footnote{\textcite{Hoeffler2017}: in 2015, of 343 journals, 26 had mandatory DAP, 110 had voluntary DAP, and 49 allowed authors to provide data and code on request, possibly in combination with the offer of a deposit. \textcite{vlaeminck2021}: of 327 journals, 50 had a mandatory DAP, 135 had a voluntary DAP, and 38 had some sort of provision on request.}. Such DAP are not necessarily a panacea, as authors who have tested them in some way note it does not guarantee obtaining all the papers' codes and data, nor does it guarantee reproducibility \parencite{Glandon2010,Hoeffler2017,Stodden2018}.

More recently, journals and societies have appointed ``data editors'' responsible for monitoring compliance. The Canadian Economic Association appointed a data editor  in 2017. The \ac{AEA} appointed one of this article's authors as data editor in 2018  \parencite{10.1257/pandp.108.745} and subsequently updated its data and code availability policy. Similar positions have been created at the Review of Economic Studies, the Economic Journal, Management Science, and most recently, Econometrica. 
%According to  \textcite{Duvendack2015} only  27 of the 333 economics journals \mc{HK}{Should we update this?} listed in the Thomson Reuters \textit{Web of Science} as of September 2013 regularly publish data and code for empirical articles, and 10 of those journals explicitly state that they publish replication studies. While that number seems low, it is higher than it was a decade earlier. More recently, the \ac{JASA} has moved towards much more stringent replication requirements \parencite{Fuentes2016-wz}, and 

In this paper,  we set out to assess how well a particular journal's ``data availability'' policy, lightly monitored as described in the next section, yields \textit{reproducible} articles. We call the monitoring ``light'', because no verification of completeness or functionality was conducted prior to publication, in contrast to the more recent monitoring conducted by dedicated data editors. In other words, we ask the question: given that authors are required to provide code and data, without pre-publication verification, is that enough to generate reproducible results?


%
% Not relevant, because a very different exercise. Leave to conclusion.
% More recently, \textcite{perignon2022} analyze the reproducibility of more than 1,000 xxxfinance papersxxx (NOT PAPERS) through a controlled experiment. They task 168 research teams to answer six research questions using the same data. In our case, some papers might be assigned more than once, but it is the minority. We set to answer a slightly different question: 


Our protocol sets a relatively high bar: Can \textit{undergraduates}, armed only with the information provided by authors on the journal website, successfully reproduce the tables and figures presented by the author in the article? Unlike \textcite{Dewald1986} and \textcite{McCullough03}, who requested data and programs from the original authors, we did not attempt any contact with authors to clarify issues that arose. While our replicators were instructed to do their best to fix any bugs or inconsistencies that they encountered, they were limited both by time and training. On the other hand, it would seem that pure computational reproducibility should be the lowest standard met by an article.
We conducted this experiment over several summers starting in 2014 and during the 2018 fall semester, using the \ac{AEJ:AE} as our source of articles.

%
% parts moved to Background
%
We find a  moderate replication success, with a reproduction rate of \input{./includes/attempted.success.rate}$\%$ overall. \input{./includes/successrate}$\%$ of articles were successfully reproduced, conditional on  data being available, with an additional \input{./includes/partialrate}$\%$ partially reproduced. Compared  to a reproduction rate of $13\%$ found by \textcite{Dewald1986} in the context of a journal (\ac{JMCB}) with no data or code availability policy, our results seem to suggest that journal policies that enhance transparency are helpful, yet not sufficient to reach full reproducibility. We further show that fully reproducible papers do not seem to benefit from a citation bonus ---  authors' reputation seems to matter the most when it comes to citations. We speculate that we mafy be in a relatively low reproducibility equilibrium because the costs of producing reproducible research (for instance in terms of time) outweigh the advantages (given it does not lead to more citations). Our contribution to the literature is thus twofold: (1) we provide an estimate of reproducibility standards of a journal that imposed, since its founding, a data availability policy; (2) we provide a rationale for authors' lack of incentives to produce reproducible research, absent journals' verification of reproducibility.


%%The tools for successful reproduction studies (which henceforth we will use to designate basic validation studies and robustness or perturbation checks) have become more widely available in recent decades,
%Short of tackling the thorny issue of how to publish replication studies, journals have attempted to provide forums for discussions of the robustness of published results.  For instance, \url{arxiv.org} and the American Economic Association have made available comment sections for any article published, so that focused scientific discussion could be made in a public forum. Unfortunately, adoption of these tools is rather weak. A scan of several years of American Economic Journals shows very little use:
%% 2014-10-09
%% awk -F, ' { sum+=$2; n+=1 } END { print sum,n } ' *csv
%% awk -F, ' { print $2 } ' *csv | sort
%out of 254 articles in the \textit{American Economic Journal: Applied Economics} (as of October 2014, and since inception), a total of 19 comments had been posted on 14 articles (the largest count was 3, and those all came from the author of the paper itself!).
%%
%One possible conclusion is that comment sections don't give the appropriate academic rewards (citability) that journal articles or similar venues would provide.
%Even when discussion does occur in other (less formal) forums, it can often become acrimonious, which can discourage both junior and senior academics from participating.




%\mc{HK}{Lars, this paragraph might be too speculative/irelevant. It's just my attempt to explain which part of the replication crisis our work best addresses. Delete if necessary.} \textcite{Christensen2018} identify several problems with the current scientific enterprise that help to explain the failure of many research findings to replicate - from researcher fraud and publication bias to simple sampling variation. Our focus on a narrow or minimal sense of reproducibility addresses most directly the issue of ``specification searching" \parencite{Leamer1983,Lalonde1986} or ``the garden of forking paths" \parencite{Gelman2013}, in which researchers make numerous analytical decisions in the process of, say, data cleaning or model selection, to arrive at an ``interesting" result, either consciously or unconsciously. The presence of computer programs that reliably reproduce the reported results can help to make explicit these various decisions taken during the research process, and help to explain why independent replications, which inevitably make different decisions, fail.

%In this article,  we set out to assess how well a particular journal's ``data availability'' policy, combined with light enforcement, yields reproducible articles. All articles stem from a small selection of \ac{AEA} journals, so we can therefore begin to assess cross-journal variation in enforcement or details of such policies. Our protocol is set with a relatively high bar: can undergraduates, armed only with the information provided by authors on the journal website, successfully reproduce the tables and figures presented by the author in the article? Unlike \textcite{Dewald1986} and \textcite{McCullough03}, who requested data and programs from the original authors, we did not attempt any contact with authors to clarify any issues that arise, and while  our replicators are instructed to do their best to fix any bugs or inconsistencies that they encounter, they are limited both by time and training.

%
%\footnote{The choice of years is quite arbitrary. We started with the 2013 volume in the summer of 2014, then added two additional, earlier years in the summer of 2015, to better capture post-publication outcomes such as citations.}
%

%We conducted this experiment over several summers and during the 2018 Fall semester, using four journals produced by the \ac{AEA}. For the first three summers from 2014-2016, we focussed soldely on the \ac{AEJ:AE} as our source of articles. We chose the \ac{AEJ:AE} primarily for two reasons. First, because of the empirical nature of its articles and its policy of publishing papers ``only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication," we expect that nearly all articles have some empirical component. Second, while other journals may also have theoretical or more complex empirical papers, using a variety of software, we wanted articles to be reproducible by the undergraduate student armed with knowledge of Stata and Matlab only. However, nothing in the methodology used in this paper was specific to the \ac{AEJ:AE}, and we later expanded our sample of articles to the \ac{AEJ:Mic}, the \ac{AEJ:Mac} and the \ac{AER}. Part of the motivation here is also to test the feasibility of ``pre-publication verification'' similar to what is done at the \ac{AJPS} \parencite{JacobyShouldJournalsBe2017,Christian2018}



We start by describing, in Section~\ref{sec:methodology}, our methods of selection, analysis, and reproduction. We present our results in Section~\ref{sec:results} before concluding in Section~\ref{sec:discussion}.
