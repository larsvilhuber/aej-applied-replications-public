% Background
% 
% in part in response to Ref 1, Q2 
%
The focus of the exercise is to assess whether availability of replication packages, in compliance with a posted data availability policy, leads to reproducibility. It is thus worthwhile to briefly discuss the policy as it was applied at AEA journals. 

The policy was first announced by \citet{bernanke2004}. Authors would provide ``in electronic form, data and code sufficient to permit replication [sic].'' It is unknown when the policy was separately posted on the AEA's website, but the policy as of 2018  \citep{American_Economic_Association2008-wayback} is largely consistent with the original (very brief) statement. Authors would email (or upload to an FTP server) ``data set(s) and programs used to run the final models'', including a Readme PDF with instructions. Exemptions for ``proprietary'' data needed to be approved by the journal's editor. The American Economic Journals launched in 2009, and applied this policy from the outset. Exemptions are (at least in later years, and through 2020) tabulated in the annual editors' reports \citep[see f.i.][]{mas2019}.\footnote{We briefly touch on assessed reasons for exemptions in Section~\ref{sec:results}.} 

We chose the \ac{AEJ:AE}  for two reasons. First, because of the empirical nature of its articles and its policy of publishing papers ``only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication" \citep{American_Economic_Association2008-wayback}. As we outline in Section~\ref{sec:methodology}, we wanted replication materials to be present for all articles in the journal, without needing to contact authors. The \ac{AEJ:AE} applied the AER's data availability policy from its creation in 2009. From past experience of this team \citep{vilhuber2020a} and others \citep[e.g.][]{Dewald1986,McCullough03,Stodden2018}, requesting materials from authors is fraught with non-response. 

%we expect that nearly all articles have some empirical component.
The second reason for the choice of the \ac{AEJ:AE} was based on feasibility. A cursory review of the articles on the \ac{AEJ:AE} website suggested that articles would be less complex than articles in the \ac{AER} or Econometrica, including by using mostly high-level software commonly used in economics, such as Stata and MATLAB.%
%
\footnote{See \citet{VilhuberAEAPap.Proc.2020} for a distribution of software used in replication packages deposited by authors in AEA journals from 1999 to 2018.}
% while other journals may also have theoretical or more complex empirical papers, using a variety of software, 
We wanted articles to be reproducible by undergraduate, students who are mostly armed with knowledge of Stata and MATLAB.%
%
\footnote{In practice, all articles in \ac{AEJ:AE} use Stata for most of the analysis, but some use additional softwares. We address this in Section~\ref{sec:results} in more details.}
However, nothing in the methodology used in this paper is specific to the \ac{AEJ:AE}, and could be applied to other journals. 

An additional, though not crucial, motivation, was to test the feasibility of ``pre-publication verification'' at AEA journals, similar to what was, at the time, done at the \ac{AJPS} \parencite{JacobyShouldJournalsBe2017,Christian2018}. The results obtained from the exercise reported in this article ultimately lead to the implementation of pre-publication verification at the \ac{AEA} journals \parencite{10.1257/pandp.109.718,vilhuber2022a}.

The exercise reported here differs somewhat from many other articles in the space of reproducibility and replicability. Some author teams have attempted to replicate, rather than computationally reproduce, published articles, in particular in the field of behavioral economics and psychology \citep{opensciencecollaboration2015b,camerer2016,camerer2018}. Others have taken a less structured approach to computational reproducibility, by simply attempting to run any code within a replication package and check for failure \citep{trisovic2021,wang2020}. Finally, others have conducted replication exercises that were designed to identify cross-researcher degrees of freedom \citep{menkveld2023,huntington-klein2021}.



How does our exercise compare with and differ from similar exercises conducted in the past? 
\textcite{Dewald1986} attempted to reproduce 54 articles from the \ac{JMCB}, a few years after \ac{JMCB} had introduced a pioneering DAP. The policy was so pioneering that many authors did not comply: Only 49 out of 65 (75\%) provided replication materials to the journal upon request, despite the policy requiring it. They report a reproduction rate of  7 out of 54 (13\%) articles, though they only attempted to actually compute the results for nine articles, absent complete data and code for others (7 out of 9, 78\%). Only two were perfectly reproduced (either 2 out of 54, or 2 out of 9). In later work using the same journal, \textcite{McCullough2006} found only 14 of 196 (7.1\%) articles selected from the \ac{JMCB} that should have had archives were actually reproducible. Only 69 articles actually had replication archives, some of which were incomplete. Conditional on having code and data, they therefore succeeded in fully reproducing 14 out of 62 (22.6\%), though they did not have sufficient computational resources to test the reproducibility of seven others.

%2 Our paper studies the effectiveness of a DAP based on archives through a rigorous and large-scale reproducibility exercise.
This paper is not the first assessment of the AEA's DAP. In 2008, six PhD students attempted to reproduce 39 empirical articles out of 135  papers published in the AER between 2006 and 2008 \parencite{Glandon2010}. Out of 39 articles, 11 were based on proprietary data and only 9 papers were actually assessed for reproducibility. However, they also assessed completeness of the provided packages, and estimated that 95\% would be reproducible without help from the authors if data were accessible. By the replicators' assessment, none were perfectly reproducible, but more than half (five out of nine) had enough information that substantial effort to fill in the blanks would yield a perfect replication. The other four had immaterial discrepancies between the results and the published paper. 

\textcite{ChangLi2015}, using slightly different methodology in selection, selected all articles from 13 well-regarded economics journals, including the AER, satisfying certain criteria (empirical paper using data on U.S. gross domestic product), and successfully reproduced the results of only 22 of 67 papers (32.8\%). This seems to suggest that journal policies to enhance publications are helpful, but insufficient to foster reproducibility. Most closely related may be \textcite{greiner2023}, who enrolled 700 (unpaid) replicators for nearly 500 articles previously published in \textit{Management Science}, both before and after the implementation of a data and code availability policy. Conditional on having data, code, and IT infrastructure in place, they were able to reproduce 95\% of articles after implementation of the policy. They find that introduction of the data and code availability policy both increased availability and quality of replication packages. 


In a related exercise, \citet{Stodden2018} used articles, including in economics, published in \textit{Science} as the basis of their analysis. Given Science's policy, they had to request data for 180 (88\%) of the articles in their sample by contacting authors.  They only managed to obtain data and code for 89 (43.6\%) of their sample, and assessed 56 of these. In the end, they only attempted reproduction of 22 (10.8\%), for which 95\% were reproducible. 

A similar exercise, albeit not focused on published articles, is \citet{perignon2022}. In their case, they were able to re-purpose the 163 replication packages generated by the \citet{menkveld2023} project, and assess reproducibility of the code used to produce the 6 research questions of that project. It is a much narrower exercise, as all researchers who participated came from a small subfield of finance, and accomplished the programming task within a much more limited timeframe than the typical economics paper. At the same time, it is also more complex, as the choice of software varied more widely within this narrowly focused field than in the entire \ac{AEJ:AE} corpus, which had to be mastered by a single replicator. The lack of field diversity, topics, and replicator skill diversity allow a clean focus on the researcher skills leading to lack of reproducibility, something which in our context will be somewhat obscured by the intersection of researcher programming skills and replicator execution skills.
