


In this paper, we carried out a large scale reproduction exercise of replication packages from a journal with a data availability policy that requires deposit of data and code. Out of \input{includes/aejdoistotal} articles published during the period under consideration, we assessed  \input{includes/total.assessed} articles. All articles provided some materials. We excluded articles that required confidential or proprietary data, or that required the replicator to otherwise obtain the data, either at initial assessment or if discovered during the reproduction attempt. 
%
We attempted to reproduce \input{includes/total.attempted.nonconf} articles, and were able to fully reproduce the results of \input{includes/attempted.success}. A further \input{includes/attempted.partial} were partially reproduced. The overall reproduction rate, depending on how one counts the partially reproduced, is either $R_{{full},{nonconf}} = $\input{includes/successrate}$\%$ or  $R_{{partial},{nonconf}} = $\input{includes/fullpartialrate}\% of articles we attempted to reproduce. We have no information on the reproducibility of articles when data acquisition was too onerous from this exercise. When articles were not reproduced, the  unexplained absence of data, without indication that it was due to confidentiality was the most frequent explanation. 
%We find no effect, for the articles in this journal, of actual reproducibility on future citations.

How do these results compare with and differ from similar exercises conducted by other authors? %Similar exercises have been performed before \parencite{McCullough2007,McCullough2006}, with poor results:
\textcite{Dewald1986} found that only 7 out of 54 (13\%) articles from the \ac{JMCB} were able to be reproduced $R_{{full},{assessed}}$. 
%
In later work using the same journal, \textcite{McCullough2006} found only 14 of 196 ($R_{{full},{assessed}} =$7.1\%) articles selected from the \ac{JMCB} were reproducible. However,  \textcite{McCullough2006} note that only 62 reproductions were attempted, which yields instead a reproduction rate of $R_{{full},{nonconf}} = 22.6\%$. 
\textcite{ChangLi2015}, using slightly different methodology in selection, selected all articles from 13 well-regarded economics journals satisfying certain criteria (empirical paper using data on U.S. gross domestic product), and successfully reproduced the results of 22 of 67 papers ($R_{{full},{nonconf}} = 32.8\%$). \textcite{greiner2023}, using faculty and graduate students as replicators, achieved a reproduction rate of ($R_{{full},{nonconf}} = 95.3\%$), although it is not clear how different their classification of ``largely reproduced'' is from our classification of ``partially reproduced.'' They also found that for 29\% of articles, replicators could not obtain data, which compares to \input{./includes/excluderate}\% in our case, though our replicators were asked to obtain no data at all. Broadly, after introduction of the data and code availability policy, the reasons for non-reproducibility are similar to the ones we find: the dominant reason is absence of access to data, with various code, information, or software issues relevant in a few cases.

We note that authors in general must have been aware of the policy at the AEJ:AE before submitting: The main journal of the AEA (the American Economic Review) had been requiring data and code for four years by the time the \ac{AEJ:AE} was first published, and the journal from the start required such materials. We  assume that the incentive effect must be higher at this journal than at journals that make provision of such materials voluntary (evidenced \textit{inter alia} by \textcite{greiner2023}), or simply encourage it, and that the reproduction rate of replication packages that we observed may constitute an upper limit when compared to journals with a looser data and code availability policy.

On the other hand, we intentionally used replicators from the lower part of the skill distribution. More skilled replicators, armed with a Ph.D. and years of experience, might be able to reproduce more articles, by fixing minor bugs or by being more skilled in filling in gaps in the code with textual information from the paper \parencite[see ][]{greiner2023}. Yet we emphasize that there is no reason why such gaps should exist, or such bugs would need to be fixed. 

The articles considered were relatively recent, and trying to reproduce papers even just a few years older might present more difficulties related to differences in software version or unavailability of data previously accessed over the internet. To assess the authenticity of the results, we would ideally use the same software version used by the authors of an article, but such software is often difficult - and, in some cases, impossible - to find or run. Most authors did not provide software version information and, to the best of our knowledge, the journals did not, at the time, attempt to capture this information from authors.%
\footnote{\textcite{McCullough2006} suggested that this be provided. The recommended README created by multiple data editors \parencite{templateREADMEv1.0,templateREADMEv1.1} and required by this journal asks for such information.}
% In economics, it is highly unusual to provide the software version used for the results obtained by the author, and we did not collect this information.
%However, based on time-lag to publication, and the age of the articles, we expect that multiple versions of each software lie between when the authors ran their programs and when our team ran the programs. For instance, Mathworks updates their software distribution of MATLAB twice a year - for an article published in 2010, which we attempted to reproduce in 2015, it is likely that the version of MATLAB used by the author was released in 2008 or 2009, at least six years before we attempted to reproduce the article.
Our analysis highlighted that complex changes to the code were sometimes required to reproduce the papers, and documentation was lacking or inadequate. This is in line with the assessment of reproducibility conducted by \textcite{trisovic2021}, who analyzed more than 2,000 replication packages in the Harvard Dataverse that used R, and found that 60$\%$ of R files crashed even after some small correction such as directory changes or packages installation (58$\%$ when restricting to data from journals). 
%
% Citation bonus
%

We also show that reproducibility of papers does not appear to provide a citation bonus. This may appear to be disappointing, given that researchers should be able to more easily build on state-of-the-art research when such research is  transparent and easily reproducible. 
%Part of the reason could be that it is sometimes easier to rewrite codes. 
Other authors have found a positive citation impact of policies \citep{gleditsch2003,Hoeffler2017}. However, their findings are generally obtained when comparing journals with and without easily observable (and well-known) data and code availability policies, with potential differences in journal quality also affecting citation levels. We only compared articles from the same journal for which we attempted reproduction, something that is not easily observable and is costly to obtain. It may thus not be particularly surprising that there is more of a reputation effect (through prior publications and the h-index) than of the reproducibility of a specific article.

We interpret the relatively high reproduction rate as indicative of the limited success of data availability policies. While such policies reduce the likelihood of materials (data and/or code) not being available, they do not ensure that such materials are functional. The large number of complex code changes necessary to obtain the results reported here are more suggestive of a need to improve technical skills of authors. Testing code prior to publication will further reduce such issues. Testing can occur internally within research groups or by using outside services \citep{perignon2019a}, before submission. It can also be conducted by active data editors at journals, such as the aforementioned journals in economics, but also in political science. Finally, as long as the materials can be corrected where readers expect to find them, for instance by updating them on journal repositories, the use of post-publication critique, as part of general academic discourse, or through structured activities \citep{brodeur2023}, can further enhance the reproducibility of articles in economics.